{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4749b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729092c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install videollama\n",
    "# !cd /code/SemEvalParticipants/semeval/experiments/belikova/videollama/VideoLLaMA; pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfe94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from backbone import VideoLLAMABackbone\n",
    "from video_llama.processors import AlproVideoTrainProcessor, AlproVideoEvalProcessor\n",
    "from video_llama.processors.video_processor import load_video\n",
    "from video_llama.models.ImageBind.data import load_and_transform_audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/code/SemEvalParticipants/semeval/experiments/belikova/videollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c36d8d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d7761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        self.attention_weights = nn.Linear(input_dim, attention_dim)\n",
    "        self.context_vector = nn.Linear(attention_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_scores = self.context_vector(torch.tanh(self.attention_weights(x)))\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        weighted_average = torch.sum(x * attention_weights, dim=1)\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "class MultimodalClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            video_embedding,\n",
    "            audio_embedding,\n",
    "            text_embedding,\n",
    "            input_dim=5120,\n",
    "            hidden_dim=512,\n",
    "            attention_dim=128,\n",
    "            num_classes=7,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.video_embedding = video_embedding\n",
    "        # self.audio_embedding = audio_embedding\n",
    "        self.text_embedding = text_embedding\n",
    "        \n",
    "        self.projections = nn.ModuleList([nn.Linear(input_dim, hidden_dim) for _ in range(2)])\n",
    "        self.attention_layers = nn.ModuleList([Attention(hidden_dim, attention_dim) for _ in range(2)])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        \n",
    "        self.train_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average=\"macro\")\n",
    "        self.val_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average=\"macro\")\n",
    "        self.test_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, text, video, audio):\n",
    "        embeddings = [\n",
    "            self.text_embedding(text),\n",
    "            self.video_embedding(video),\n",
    "            # self.audio_embedding(audio),\n",
    "        ]\n",
    "        projections = [\n",
    "            att(p(e.float()))\n",
    "            for att, p, e in zip(self.attention_layers, self.projections, embeddings)\n",
    "        ]\n",
    "        concat_features = torch.cat(projections, dim=1)\n",
    "        logits = self.classifier(concat_features)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, audio, text = batch[\"video\"], batch[\"audio\"], batch[\"text\"]\n",
    "        labels = batch[\"label\"]\n",
    "        logits = self(text, video, audio)\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\"train_f1\", self.train_f1(preds, labels), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_accuracy(preds, labels), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, audio, text = batch[\"video\"], batch[\"audio\"], batch[\"text\"]\n",
    "        labels = batch[\"label\"]\n",
    "        logits = self(text, video, audio)\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log('val_f1', self.val_f1(preds, labels), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_accuracy(preds, labels), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, audio, text = batch[\"video\"], batch[\"audio\"], batch[\"text\"]\n",
    "        labels = batch[\"label\"]\n",
    "        logits = self(text, video, audio)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\"test_f1\", self.test_f1(preds, labels), on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_accuracy(preds, labels), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.val_f1.reset()\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.test_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114b8b3",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc301bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emotions = [\n",
    "    \"surprise\",\n",
    "    \"fear\",\n",
    "    \"sadness\",\n",
    "    \"neutral\",\n",
    "    \"joy\",\n",
    "    \"anger\",\n",
    "    \"disgust\",\n",
    "]\n",
    "\n",
    "emotions2labels = {em: i for i, em in enumerate(all_emotions)}\n",
    "labels2emotions = {i: em for i, em in enumerate(all_emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ecea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemEvalDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            data_name=\"dim/SemEval_training_data_emotions\",\n",
    "            root=\"/code/data/video_with_audio\",\n",
    "            split=\"train\",\n",
    "            num_frames=8,\n",
    "            resize_size=224,\n",
    "            tokenizer_name=ROOT + \"/ckpt/llama-2-13b-chat-hf\",\n",
    "        ):\n",
    "        self.root = root\n",
    "        self.annotation = load_dataset(data_name, split=split)\n",
    "        self.num_frames = num_frames\n",
    "        self.resize_size = resize_size\n",
    "        if split == \"train\":\n",
    "            self.transform = AlproVideoTrainProcessor(\n",
    "                image_size=resize_size,\n",
    "                n_frms=num_frames,\n",
    "            ).transform\n",
    "        else:\n",
    "            self.transform = AlproVideoEvalProcessor(\n",
    "                image_size=resize_size,\n",
    "                n_frms=num_frames,\n",
    "            ).transform\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name, use_fast=False)\n",
    "        self.tokenizer.pad_token = self.tokenizer.unk_token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "        \n",
    "    def __getitem__(self, index, num_retries=10, device=\"cpu\"):\n",
    "        result = {}\n",
    "        for _ in range(num_retries):\n",
    "            sample = self.annotation[index]\n",
    "            video_path = \"/\".join([self.root, sample[\"video_name\"]])\n",
    "            try:\n",
    "                result[\"video\"] = self.transform(\n",
    "                    load_video(\n",
    "                        video_path=video_path,\n",
    "                        n_frms=self.num_frames,\n",
    "                        height=self.resize_size,\n",
    "                        width=self.resize_size,\n",
    "                        sampling =\"uniform\",\n",
    "                        return_msg = False,\n",
    "                    )\n",
    "                )\n",
    "                result[\"text\"] = self.tokenizer(\n",
    "                    sample[\"text\"],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"longest\",\n",
    "                    max_length=512,\n",
    "                    truncation=True,\n",
    "                ).input_ids[0]\n",
    "                result[\"audio\"] = load_and_transform_audio_data(\n",
    "                    [video_path],\n",
    "                    device=device,\n",
    "                    clips_per_video=self.num_frames,\n",
    "                )[0]\n",
    "                result[\"label\"] = emotions2labels[sample[\"emotion\"]]\n",
    "                assert result[\"video\"].shape[1] == self.num_frames == result[\"audio\"].shape[0]\n",
    "            except Exception as e:\n",
    "                index = random.randint(0, len(self) - 1)\n",
    "                continue\n",
    "            break\n",
    "        else:  \n",
    "            raise RuntimeError(f\"Failed to fetch sample after {num_retries} retries.\")\n",
    "        return result\n",
    "    \n",
    "    def collater(self, instances):\n",
    "        text_ids = [instance[\"text\"] for instance in instances]\n",
    "        text_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            text_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        \n",
    "        batch = {\n",
    "            \"video\": torch.stack([instance['video'] for instance in instances]),\n",
    "            \"text\": text_ids,\n",
    "            \"audio\": torch.stack([instance['audio'] for instance in instances]),\n",
    "            \"label\": torch.tensor([instance['label'] for instance in instances]),\n",
    "        }\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32838939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemEvalDataset(split=\"train\")\n",
    "val_dataset = SemEvalDataset(split=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=train_dataset.collater)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=4, collate_fn=val_dataset.collater)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f8b46",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f8ef2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "max_epochs = 20\n",
    "output_path = ROOT + \"/output/emo_classification_model_0.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2904da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "config = OmegaConf.load(ROOT + \"/configs/backbone.yaml\")\n",
    "video_backbone = VideoLLAMABackbone.from_config(config)\n",
    "video_backbone.to(device)\n",
    "video_embedding = video_backbone.encode_videoQformer\n",
    "audio_embedding = video_backbone.encode_audioQformer\n",
    "\n",
    "text_embedding = nn.Embedding(32000, 5120, padding_idx=0, _freeze=True)\n",
    "text_embedding.load_state_dict(torch.load(ROOT + \"/ckpt/llama_embedding.pth\"))\n",
    "text_embedding.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63112e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    name=\"multimodal_classification_1\",\n",
    "    project=\"emotion_analysis\"\n",
    ")\n",
    "\n",
    "model = MultimodalClassifier(\n",
    "    video_embedding,\n",
    "    audio_embedding,\n",
    "    text_embedding,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=-1,\n",
    "    logger=wandb_logger)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.save_checkpoint(output_path)\n",
    "\n",
    "wandb_logger.experiment.save(output_path)\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168aa130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/julia-bel/emotion_analysis/runs/z69htvq8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
