{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install videollama  \n",
    "`!pip install semeval/experiments/belikova/videollama/VideoLLaMA`\n",
    "\n",
    "Download all weights to `semeval/experiments/belikova/videollama/ckpt`\n",
    "- https://huggingface.co/dim/SemEvalParticipants_models/blob/main/belikova/llama_embedding.pth  \n",
    "- https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-13B-Finetuned/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from semeval.experiments.belikova.videollama.models import (\n",
    "    VideoLLAMABackbone,\n",
    "    EmotionCausalClassifier,\n",
    ")\n",
    "from semeval.experiments.belikova.videollama.data import (\n",
    "    EmotionCausalDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        self.attention_weights = nn.Linear(input_dim, attention_dim)\n",
    "        self.context_vector = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = self.context_vector(torch.tanh(self.attention_weights(x)))\n",
    "        attention_weights = F.softmax(attention_scores, dim=2)\n",
    "        weighted_average = torch.sum(x * attention_weights, dim=2)\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "class CausalClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        input_dim=5120,\n",
    "        attention_dim=128,\n",
    "        modality_embedding_dim=256,\n",
    "        emotion_embedding_dim=7,\n",
    "        lstm_hidden_dim=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.modalities = embeddings.keys()\n",
    "\n",
    "        self.projections = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, modality_embedding_dim * 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(modality_embedding_dim * 2, modality_embedding_dim),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for _ in range(len(embeddings))\n",
    "            ]\n",
    "        )\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                Attention2d(modality_embedding_dim, attention_dim)\n",
    "                for _ in range(len(embeddings))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.emotion_bilstm = nn.LSTM(\n",
    "            modality_embedding_dim * len(self.modalities) + emotion_embedding_dim,\n",
    "            lstm_hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.causal_bilstm = nn.LSTM(\n",
    "            modality_embedding_dim * len(self.modalities),\n",
    "            lstm_hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.f1 = torchmetrics.F1Score(\n",
    "            task=\"binary\", multidim_average=\"global\", ignore_index=-1\n",
    "        )\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"binary\", multidim_average=\"global\", ignore_index=-1\n",
    "        )\n",
    "\n",
    "    def forward(self, modality_embeddings, utterance_lengths, emotion_embeddings):\n",
    "        projections = [\n",
    "            attention(project(embedding.float()))\n",
    "            for attention, project, embedding in zip(\n",
    "                self.attentions, self.projections, modality_embeddings\n",
    "            )\n",
    "        ]\n",
    "        utterance_embeddings = torch.cat(projections, dim=2)\n",
    "\n",
    "        causal_packed_utterances = rnn_utils.pack_padded_sequence(\n",
    "            utterance_embeddings,\n",
    "            utterance_lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        causal_packed_output, _ = self.causal_bilstm(causal_packed_utterances)\n",
    "        causal_utterances, _ = rnn_utils.pad_packed_sequence(\n",
    "            causal_packed_output, batch_first=True\n",
    "        )\n",
    "\n",
    "        emotion_packed_utterances = rnn_utils.pack_padded_sequence(\n",
    "            torch.cat((utterance_embeddings, emotion_embeddings), dim=2),\n",
    "            utterance_lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        emotion_packed_output, _ = self.emotion_bilstm(emotion_packed_utterances)\n",
    "        emotion_utterances, _ = rnn_utils.pad_packed_sequence(\n",
    "            emotion_packed_output, batch_first=True\n",
    "        )\n",
    "\n",
    "        logits = torch.bmm(emotion_utterances, causal_utterances.transpose(1, 2))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        range_tensor = torch.arange(num_utterances).expand(batch_size, num_utterances)\n",
    "        mask = (range_tensor < utterance_lengths.unsqueeze(1)).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.float(), weight=mask)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"train_f1\", self.f1(preds, labels))\n",
    "        self.log(\"train_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        range_tensor = torch.arange(num_utterances).expand(batch_size, num_utterances)\n",
    "        mask = (range_tensor < utterance_lengths.unsqueeze(1)).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.float(), weight=mask)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"val_f1\", self.f1(preds, labels))\n",
    "        self.log(\"val_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"test_f1\", self.f1(preds, labels))\n",
    "        self.log(\"test_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention1d(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        self.attention_weights = nn.Linear(input_dim, attention_dim)\n",
    "        self.context_vector = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = self.context_vector(torch.tanh(self.attention_weights(x)))\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        weighted_average = torch.sum(x * attention_weights, dim=1)\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "class EmotionClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        input_dim=5120,\n",
    "        hidden_dim=512,\n",
    "        attention_dim=128,\n",
    "        num_classes=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.modalities = embeddings.keys()\n",
    "\n",
    "        self.projections = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())\n",
    "                for _ in range(len(embeddings))\n",
    "            ]\n",
    "        )\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Attention1d(hidden_dim, attention_dim) for _ in range(len(embeddings))]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=num_classes\n",
    "        )\n",
    "        self.f1 = torchmetrics.F1Score(\n",
    "            task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        projections = [\n",
    "            attention(project(embedding.float()))\n",
    "            for attention, project, embedding in zip(\n",
    "                self.attentions, self.projections, embeddings\n",
    "            )\n",
    "        ]\n",
    "        concat_features = torch.cat(projections, dim=1)\n",
    "        logits = self.classifier(concat_features)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        embeddings = [self.embeddings[mod](batch[mod]) for mod in self.modalities]\n",
    "        logits = self(embeddings)\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\n",
    "            \"train_f1\",\n",
    "            self.f1(preds, labels),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_acc\",\n",
    "            self.accuracy(preds, labels),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embeddings = [self.embeddings[mod](batch[mod]) for mod in self.modalities]\n",
    "        logits = self(embeddings)\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\n",
    "            \"val_f1\",\n",
    "            self.f1(preds, labels),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_acc\",\n",
    "            self.accuracy(preds, labels),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        embeddings = [self.embeddings[mod](batch[mod]) for mod in self.modalities]\n",
    "        logits = self(embeddings)\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.log(\n",
    "            \"test_f1\",\n",
    "            self.f1(preds, labels),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_acc\",\n",
    "            self.accuracy(preds, labels),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "root = \"/code/SemEvalParticipants/semeval/experiments/belikova/videollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = [\"text\", \"video\"]\n",
    "train_dataset = EmotionCausalDataset(split=\"train\", modalities=modalities)\n",
    "val_dataset = EmotionCausalDataset(split=\"test\", modalities=modalities)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=train_dataset.collater,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=3,\n",
    "    num_workers=4,\n",
    "    collate_fn=val_dataset.collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "config = OmegaConf.load(root + \"/configs/backbone.yaml\")\n",
    "video_backbone = VideoLLAMABackbone.from_config(config)\n",
    "video_backbone.to(device)\n",
    "embeddings[\"video\"] = video_backbone.encode_videoQformer\n",
    "# audio_embedding = video_backbone.encode_audioQformer\n",
    "\n",
    "text_embedding = nn.Embedding(32000, 5120, padding_idx=0)\n",
    "text_embedding.load_state_dict(\n",
    "    torch.load(root + \"/ckpt/llama_embedding.pth\")\n",
    ")\n",
    "text_embedding.to(device)\n",
    "embeddings[\"text\"] = text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "emotion_model = EmotionClassifier(\n",
    "    {m: None for m in modalities},\n",
    "    hidden_dim=512,\n",
    "    attention_dim=128,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "causal_model = CausalClassifier(\n",
    "    {m: None for m in modalities},\n",
    "    attention_dim=128,\n",
    "    modality_embedding_dim=512,\n",
    "    lstm_hidden_dim=128,\n",
    "    emotion_embedding_dim=num_classes,\n",
    ")\n",
    "emotion_model.to(device)\n",
    "causal_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "output_path = root + \"/output/joint_classification_model_0.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    name=\"joint_classification_0\", project=\"emotion_analysis\"\n",
    ")\n",
    "\n",
    "model = EmotionCausalClassifier(\n",
    "    embeddings,\n",
    "    causal_classifier=causal_model,\n",
    "    emotion_classifier=emotion_model,\n",
    "    hidden_dim=512,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, accelerator=\"gpu\", devices=-1, logger=wandb_logger\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.save_checkpoint(output_path)\n",
    "\n",
    "wandb_logger.experiment.save(output_path)\n",
    "wandb_logger.experiment.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
