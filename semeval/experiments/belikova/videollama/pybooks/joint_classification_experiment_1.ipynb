{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic bilstm architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class CausalClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        input_dim=4096,\n",
    "        attention_dim=512,\n",
    "        modality_embedding_dim=1024,\n",
    "        emotion_embedding_dim=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.modalities = embeddings.keys()\n",
    "\n",
    "        self.projections = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, modality_embedding_dim * 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(modality_embedding_dim * 2, modality_embedding_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                )\n",
    "                for _ in range(len(embeddings))\n",
    "            ]\n",
    "        )\n",
    "        self.emotion_linear = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                modality_embedding_dim * len(self.modalities), modality_embedding_dim\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(modality_embedding_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        self.causal_bilstm = nn.LSTM(\n",
    "            modality_embedding_dim * len(self.modalities),\n",
    "            attention_dim // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.emotion_bilstm = nn.LSTM(\n",
    "            modality_embedding_dim * len(self.modalities),\n",
    "            attention_dim // 2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(attention_dim, attention_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(attention_dim // 2, emotion_embedding_dim),\n",
    "        )\n",
    "\n",
    "        self.causal_classifier = nn.Sequential(\n",
    "            nn.Linear(attention_dim * 2, attention_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(attention_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "        self.f1 = torchmetrics.F1Score(\n",
    "            task=\"binary\", multidim_average=\"global\", ignore_index=-1\n",
    "        )\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"binary\", multidim_average=\"global\", ignore_index=-1\n",
    "        )\n",
    "\n",
    "    def forward(self, modality_embeddings):\n",
    "        projections = [\n",
    "            project(embedding.float())\n",
    "            for project, embedding in zip(self.projections, modality_embeddings)\n",
    "        ]\n",
    "\n",
    "        utterance_embeddings = torch.cat(projections, dim=2)\n",
    "        emotion_embeddings, _ = self.emotion_bilstm(utterance_embeddings)\n",
    "        causal_embeddings, _ = self.causal_bilstm(utterance_embeddings)\n",
    "        \n",
    "        batch_size, seq_len, _ = emotion_embeddings.shape\n",
    "        emotion_utterances = emotion_embeddings.unsqueeze(2).expand(-1, -1, seq_len, -1)\n",
    "        causal_utterances = causal_embeddings.unsqueeze(1).expand(-1, seq_len, -1, -1)\n",
    "        combined_utterances = torch.cat((emotion_utterances, causal_utterances), dim=-1)\n",
    "        causal_logits = self.causal_classifier(combined_utterances).view(\n",
    "            batch_size, seq_len, seq_len\n",
    "        )\n",
    "        emotion_logits = self.emotion_classifier(emotion_embeddings)\n",
    "\n",
    "        return emotion_logits, causal_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        range_tensor = torch.arange(num_utterances).expand(batch_size, num_utterances)\n",
    "        mask = (range_tensor < utterance_lengths.unsqueeze(1)).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.float(), weight=mask)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"train_f1\", self.f1(preds, labels))\n",
    "        self.log(\"train_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        range_tensor = torch.arange(num_utterances).expand(batch_size, num_utterances)\n",
    "        mask = (range_tensor < utterance_lengths.unsqueeze(1)).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.float(), weight=mask)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"val_f1\", self.f1(preds, labels))\n",
    "        self.log(\"val_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"test_f1\", self.f1(preds, labels))\n",
    "        self.log(\"test_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
