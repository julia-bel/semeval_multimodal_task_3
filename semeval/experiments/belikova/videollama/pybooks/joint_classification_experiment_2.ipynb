{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder with speaker embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len: int = 100, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerWithSpeakerEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        d_model=512,\n",
    "        nhead=2,\n",
    "        num_encoder_layers=2,\n",
    "        dim_feedforward=512,\n",
    "        num_speakers=20,\n",
    "        max_position_embeddings=100,\n",
    "    ):\n",
    "        super(TransformerWithSpeakerEmbedding, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_speakers = num_speakers\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.speaker_embedding = nn.Embedding(num_speakers, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_position_embeddings)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_encoder_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, src, speaker_ids):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "\n",
    "        if speaker_ids is not None:\n",
    "            speaker_embeddings = self.speaker_embedding(speaker_ids)\n",
    "            src = src + speaker_embeddings\n",
    "\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CausalClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        input_dim=4096,\n",
    "        attention_dim=512,\n",
    "        modality_embedding_dim=1024,\n",
    "        emotion_embedding_dim=7,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.modalities = embeddings.keys()\n",
    "\n",
    "        self.projections = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_dim, modality_embedding_dim * 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(modality_embedding_dim * 2, modality_embedding_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                )\n",
    "                for _ in range(len(embeddings))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.emotion_linear = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                modality_embedding_dim * len(self.modalities), modality_embedding_dim\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(modality_embedding_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        self.causal_attention = TransformerWithSpeakerEmbedding(\n",
    "            input_dim=modality_embedding_dim * len(self.modalities),\n",
    "            d_model=attention_dim,\n",
    "        )\n",
    "\n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(attention_dim, attention_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(attention_dim // 2, emotion_embedding_dim),\n",
    "        )\n",
    "\n",
    "        self.causal_classifier = nn.Sequential(\n",
    "            nn.Linear(attention_dim * 2, attention_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(attention_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "        self.f1 = torchmetrics.F1Score(\n",
    "            task=\"binary\", multidim_average=\"global\", ignore_index=-1\n",
    "        )\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"binary\", multidim_average=\"global\", ignore_index=-1\n",
    "        )\n",
    "\n",
    "    def forward(self, modality_embeddings, speaker_ids):\n",
    "        projections = [\n",
    "            project(embedding.float())\n",
    "            for project, embedding in zip(self.projections, modality_embeddings)\n",
    "        ]\n",
    "\n",
    "        utterance_embeddings = torch.cat(projections, dim=2)\n",
    "        causal_embeddings = self.causal_attention(utterance_embeddings, speaker_ids)\n",
    "\n",
    "        emotion_embeddings = self.emotion_linear(utterance_embeddings)\n",
    "        emotion_logits = self.emotion_classifier(emotion_embeddings)\n",
    "\n",
    "        batch_size, seq_len, _ = emotion_embeddings.shape\n",
    "        emotion_utterances = emotion_embeddings.unsqueeze(2).expand(-1, -1, seq_len, -1)\n",
    "        causal_utterances = causal_embeddings.unsqueeze(1).expand(-1, seq_len, -1, -1)\n",
    "        combined_utterances = torch.cat((emotion_utterances, causal_utterances), dim=-1)\n",
    "        causal_logits = self.causal_classifier(combined_utterances).view(\n",
    "            batch_size, seq_len, seq_len\n",
    "        )\n",
    "\n",
    "        return emotion_logits, causal_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        range_tensor = torch.arange(num_utterances).expand(batch_size, num_utterances)\n",
    "        mask = (range_tensor < utterance_lengths.unsqueeze(1)).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.float(), weight=mask)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"train_f1\", self.f1(preds, labels))\n",
    "        self.log(\"train_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        range_tensor = torch.arange(num_utterances).expand(batch_size, num_utterances)\n",
    "        mask = (range_tensor < utterance_lengths.unsqueeze(1)).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.float(), weight=mask)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"val_f1\", self.f1(preds, labels))\n",
    "        self.log(\"val_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        utterance_lengths = batch[\"utterance_length\"]\n",
    "        emotion_embeddings = batch[\"emotion_embedding\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        assert all([m in batch for m in self.modalities]), \"incorrect modality input\"\n",
    "        batch_size, num_utterances = batch[self.modalities[0]].shape[:2]\n",
    "        modality_embeddings = [\n",
    "            self.embeddings[m](\n",
    "                batch[m].reshape(batch_size * num_utterances, *batch[m].shape[2:])\n",
    "            )\n",
    "            for m in self.modalities\n",
    "        ]\n",
    "        modality_embeddings = [\n",
    "            e.reshape(batch_size, num_utterances, *e.shape[1:])\n",
    "            for e in modality_embeddings\n",
    "        ]\n",
    "        logits = self(modality_embeddings, utterance_lengths, emotion_embeddings)\n",
    "\n",
    "        # Calculate metrics\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        self.log(\"test_f1\", self.f1(preds, labels))\n",
    "        self.log(\"test_accuracy\", self.accuracy(preds, labels))\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.f1.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
