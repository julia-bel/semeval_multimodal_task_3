{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-f1_af4h3 because the default path (/home/user-name-goes-here/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from gradio.themes.utils import colors, fonts, sizes\n",
    "\n",
    "from semeval.experiments.kosenko.ask_anything.video_chat2.conversation import Chat\n",
    "\n",
    "# videochat\n",
    "from semeval.experiments.kosenko.ask_anything.video_chat2.utils.config import Config\n",
    "from semeval.experiments.kosenko.ask_anything.video_chat2.utils.easydict import EasyDict\n",
    "from semeval.experiments.kosenko.ask_anything.video_chat2.models.videochat2_it import (\n",
    "    VideoChat2_it,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing VideoChat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['llama_model.base_model.model.model.embed_tokens.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.0.input_layernorm.weight', 'llama_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.1.input_layernorm.weight', 'llama_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.2.input_layernorm.weight', 'llama_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.3.input_layernorm.weight', 'llama_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.4.input_layernorm.weight', 'llama_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.5.input_layernorm.weight', 'llama_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.6.input_layernorm.weight', 'llama_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.7.input_layernorm.weight', 'llama_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.8.input_layernorm.weight', 'llama_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.9.input_layernorm.weight', 'llama_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.10.input_layernorm.weight', 'llama_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.11.input_layernorm.weight', 'llama_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.12.input_layernorm.weight', 'llama_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.13.input_layernorm.weight', 'llama_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.14.input_layernorm.weight', 'llama_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.15.input_layernorm.weight', 'llama_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.16.input_layernorm.weight', 'llama_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.17.input_layernorm.weight', 'llama_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.18.input_layernorm.weight', 'llama_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.19.input_layernorm.weight', 'llama_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.20.input_layernorm.weight', 'llama_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.21.input_layernorm.weight', 'llama_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.22.input_layernorm.weight', 'llama_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.23.input_layernorm.weight', 'llama_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.24.input_layernorm.weight', 'llama_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.25.input_layernorm.weight', 'llama_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.26.input_layernorm.weight', 'llama_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.27.input_layernorm.weight', 'llama_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.28.input_layernorm.weight', 'llama_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.29.input_layernorm.weight', 'llama_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.30.input_layernorm.weight', 'llama_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.31.input_layernorm.weight', 'llama_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'llama_model.base_model.model.model.norm.weight', 'llama_model.base_model.model.lm_head.weight'], unexpected_keys=[])\n",
      "Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "def init_model():\n",
    "    print(\"Initializing VideoChat\")\n",
    "    config_file = (\n",
    "        \"/code/semeval/experiments/kosenko/ask_anything/video_chat2/configs/config.json\"\n",
    "    )\n",
    "    cfg = Config.from_file(config_file)\n",
    "    cfg.model.vision_encoder.num_frames = 4\n",
    "    # cfg.model.videochat2_model_path = \"\"\n",
    "    # cfg.model.debug = True\n",
    "    model = VideoChat2_it(config=cfg.model)\n",
    "    model = model.to(torch.device(cfg.device))\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.0,\n",
    "    )\n",
    "    model.llama_model = get_peft_model(model.llama_model, peft_config)\n",
    "    # state_dict = torch.load(\"your_model_path/videochat2_7b_stage3.pth\", \"cpu\")\n",
    "    videochat2_model_path = \"/code/semeval/experiments/kosenko/ask_anything/video_chat2/videochat2_7b_stage3.pth\"\n",
    "    state_dict = torch.load(videochat2_model_path, \"cpu\")\n",
    "    if \"model\" in state_dict.keys():\n",
    "        msg = model.load_state_dict(state_dict[\"model\"], strict=False)\n",
    "    else:\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "    model = model.eval()\n",
    "\n",
    "    chat = Chat(model)\n",
    "    print(\"Initialization Finished\")\n",
    "    return chat\n",
    "\n",
    "\n",
    "chat = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input video shape: torch.Size([96, 224, 224])\n",
      "n_position: 6272\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 32\n",
      "Interpolate the position embedding\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dataset = json.loads(\n",
    "    open(\"./SemEval-2024_Task3/training_data/Subtask_2_train.json\").read()\n",
    ")\n",
    "\n",
    "base_video_path = \"/code/SemEval-2024_Task3/training_data/train\"\n",
    "video_paths = [\n",
    "    f\"{base_video_path}/{item['video_name']}\" for item in dataset[10][\"conversation\"]\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # image = \"semeval/experiments/kosenko/ask_anything/video_chat2/example/yoga.mp4\"\n",
    "\n",
    "    image = (\n",
    "\n",
    "        \"/code/semeval/experiments/kosenko/ask_anything/example/hitting_baseball.mp4\"\n",
    "\n",
    "    )\n",
    "\n",
    "    conv = EasyDict(\n",
    "\n",
    "        {\"system\": \"\", \"roles\": [\"Human\", \"Assistant\"], \"messages\": [], \"sep\": \"###\"}\n",
    "\n",
    "    )\n",
    "\n",
    "    img_list = []\n",
    "\n",
    "    num_segments = 32\n",
    "\n",
    "    llm_message, img_list, chat_state = chat.upload_video(\n",
    "\n",
    "        # image=image,\n",
    "        image=video_paths,\n",
    "        conv=conv,\n",
    "        img_list=img_list,\n",
    "\n",
    "        num_segments=num_segments,\n",
    "        video_prompt=\"Watch the video and predict emotion of last speaker.\",\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_ID': 11,\n",
       " 'conversation': [{'utterance_ID': 1,\n",
       "   'text': 'I mean , why should I let them meet him ? I mean , I bring a guy home , and within five minutes they are all over him . I mean , they are like ... coyotes , picking off the weak members of the herd .',\n",
       "   'speaker': 'Monica',\n",
       "   'emotion': 'disgust',\n",
       "   'video_name': 'dia11utt1.mp4'},\n",
       "  {'utterance_ID': 2,\n",
       "   'text': 'Listen . As someone who seen more than her fair share of bad beef , I will tell you : that is not such a terrible thing .',\n",
       "   'speaker': 'Paula',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia11utt2.mp4'},\n",
       "  {'utterance_ID': 3,\n",
       "   'text': 'Come on , they are your friends , they are just looking out after you .',\n",
       "   'speaker': 'Paula',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia11utt3.mp4'},\n",
       "  {'utterance_ID': 4,\n",
       "   'text': 'I know . I just wish that once , I would bring a guy home that they actually liked .',\n",
       "   'speaker': 'Monica',\n",
       "   'emotion': 'sadness',\n",
       "   'video_name': 'dia11utt4.mp4'},\n",
       "  {'utterance_ID': 5,\n",
       "   'text': 'Well , you do realise the odds of that happening are a little slimmer if they never get to meet the guy ...',\n",
       "   'speaker': 'Paula',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia11utt5.mp4'}],\n",
       " 'emotion-cause_pairs': [['1_disgust', '1'], ['4_sadness', '4']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker: Joey\n",
      "Text: Let it go , Ross .\n",
      "Emotion: neutral\n",
      "Speaker: Ross\n",
      "Text: Yeah , well , you did not know Chi Chi .\n",
      "Emotion: anger\n",
      "Speaker: Monica\n",
      "Text: Do you all promise ?\n",
      "Emotion: neutral\n",
      "Speaker: All\n",
      "Text: Yeah ! We promise ! We will be good !\n",
      "Emotion: neutral\n",
      "Speaker: Monica\n",
      "Text: Chandler ? Do you promise to be good ?\n",
      "Emotion: neutral\n",
      "Speaker: Joey\n",
      "Text: You can come in , but your filter ... tipped little buddy has to stay outside !\n",
      "Emotion: joy\n",
      "Speaker: Ross\n",
      "Text: Hey , Pheebs .\n",
      "Emotion: neutral\n",
      "Speaker: Phoebe\n",
      "Text: Dear Ms . Buffay . Thank you for calling attention to our error . We have credited your account with five hundred dollars .\n",
      "Emotion: neutral\n",
      "Speaker: Phoebe\n",
      "Text: We are sorry for the inconvenience , and hope you will accept this football phone ... as our free gift . Do you believe this ? ! Now I have a thousand dollars , and a football phone !\n",
      "Emotion: surprise\n"
     ]
    }
   ],
   "source": [
    "dialog_example = [\n",
    "    f\"\"\"Speaker: {item['speaker']}\n",
    "Text: {item['text']}\n",
    "Emotion: {item['emotion']}\"\"\"\n",
    "    for item in dataset[11][\"conversation\"]\n",
    "]\n",
    "# golden_true = dialog_example.pop()\n",
    "dialog_example = \"\\n\".join(dialog_example)\n",
    "print(dialog_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 4096])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The last \"Text\" of the speaker \"Joey\" is \"You can come in, but your filter ... tipped little buddy has to stay outside !\". Based on this, the last emotion can be disgust.\n"
     ]
    }
   ],
   "source": [
    "conv = EasyDict(\n",
    "    {\n",
    "        \"system\": \"\",\n",
    "        \"roles\": [\"Human\", \"Assistant\"],\n",
    "        \"messages\": [\n",
    "            [\"Human\", \"<Video><VideoHere></Video>\\n\"],\n",
    "            [\n",
    "                \"Human\",\n",
    "                \"\"\"\n",
    "Speaker: Joey\n",
    "Text: Let it go , Ross .\n",
    "Emotion: neutral\n",
    "Speaker: Ross\n",
    "Text: Yeah , well , you did not know Chi Chi .\n",
    "Position: 2\n",
    "Emotion: anger\n",
    "Text: Do you all promise ?\n",
    "Emotion: neutral\n",
    "Speaker: All\n",
    "Text: Yeah ! We promise ! We will be good !\n",
    "Emotion: neutral\n",
    "Speaker: Monica\n",
    "Text: Chandler ? Do you promise to be good ?\n",
    "Emotion: neutral\n",
    "Speaker: Joey\n",
    "Text: You can come in , but your filter ... tipped little buddy has to stay outside !\n",
    "---\n",
    "Speaker: Joey\n",
    "Text: Let it go , Ross .\n",
    "Emotion: neutral\n",
    "CAUSE: 1 2\n",
    "Predict last emotion based on last \"Text\" of speaker \"Joey\". \n",
    "Select from this list: surprise, fear, sadness, neutral, joy, anger, disgust. \n",
    "\"\"\",\n",
    "                # Emotion: joy\n",
    "                # Let's think step by step.\n",
    "            ],\n",
    "            # [\"Assistant\", None],\n",
    "        ],\n",
    "        \"sep\": \"###\",\n",
    "    }\n",
    ")\n",
    "max_new_tokens = 200\n",
    "num_beams = 1\n",
    "min_length = 1\n",
    "top_p = 0.9\n",
    "repetition_penalty = 1.0\n",
    "length_penalty = 1\n",
    "temperature = 1.0\n",
    "\n",
    "llm_message, llm_message_token, chat_state = chat.answer(\n",
    "    conv=conv,\n",
    "    img_list=img_list,\n",
    "    max_new_tokens=1000,\n",
    "    num_beams=1,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "llm_message = llm_message.replace(\"<s>\", \"\")  # handle <s>\n",
    "print(f\"Answer: {llm_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next emotion predicted based on the last text is disgust.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple forward with freeze img tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = EasyDict(\n",
    "    {\n",
    "        \"system\": \"\",\n",
    "        \"roles\": [\"Human\", \"Assistant\"],\n",
    "        \"messages\": [\n",
    "            [\"Human\", \"<Video><VideoHere></Video>\\n\"],\n",
    "            [\"Human\", \"Describe the following image in details and say hello.\\n\"],\n",
    "            # [\"Assistant\", None],\n",
    "        ],\n",
    "        \"sep\": \"###\",\n",
    "    }\n",
    ")\n",
    "\n",
    "conv.messages.append([conv.roles[1], None])\n",
    "embs = chat.get_context_emb(conv, img_list)\n",
    "outputs = chat.model.llama_model(\n",
    "    inputs_embeds=embs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 130, 4096])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.666666666666664"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 / 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.io import read_video\n",
    "import json\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker: Monica\n",
      "Text: I mean , why should I let them meet him ? I mean , I bring a guy home , and within five minutes they are all over him . I mean , they are like ... coyotes , picking off the weak members of the herd .\n",
      "Emotion: disgust\n",
      "Speaker: Paula\n",
      "Text: Listen . As someone who seen more than her fair share of bad beef , I will tell you : that is not such a terrible thing .\n",
      "Emotion: neutral\n",
      "Speaker: Paula\n",
      "Text: Come on , they are your friends , they are just looking out after you .\n",
      "Emotion: neutral\n",
      "Speaker: Monica\n",
      "Text: I know . I just wish that once , I would bring a guy home that they actually liked .\n",
      "Emotion: sadness\n",
      "Speaker: Paula\n",
      "Text: Well , you do realise the odds of that happening are a little slimmer if they never get to meet the guy ...\n",
      "Emotion: neutral\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/code/SemEval-2024_Task3/training_data/train/dia1utt1.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt2.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt3.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt4.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt5.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt6.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt7.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt8.mp4']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from enum import auto, Enum\n",
    "\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from semeval.experiments.kosenko.ask_anything.video_chat2.dataset.video_transforms import (\n",
    "    GroupNormalize,\n",
    "    GroupScale,\n",
    "    GroupCenterCrop,\n",
    "    Stack,\n",
    "    ToTorchFormatTensor,\n",
    ")\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.io import read_video, write_video\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# def concat_videos(video_paths):\n",
    "#     all_videos = []\n",
    "#     avg_fps_total = []\n",
    "#     for video_path in video_paths:\n",
    "#         vr = VideoReader(video_path, ctx=cpu(0))\n",
    "#         all_video = vr.get_batch(list(range(len(vr)))).numpy()\n",
    "#         all_videos.append(all_video)\n",
    "#         avg_fps_total.append(vr.get_avg_fps())\n",
    "\n",
    "#     all_videos = np.concatenate(all_videos)\n",
    "#     avg_fps_total = np.mean(avg_fps_total)\n",
    "#     write_video(\n",
    "#         \"./test.mp4\",\n",
    "#         torch.tensor(all_videos),\n",
    "#         fps=avg_fps_total,\n",
    "#     )\n",
    "\n",
    "\n",
    "# item = concat_videos(\n",
    "#     video_paths=video_paths,\n",
    "# )\n",
    "video_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/code/SemEval-2024_Task3/training_data/train/dia1utt1.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt2.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt3.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt4.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt5.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt6.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt7.mp4',\n",
       " '/code/SemEval-2024_Task3/training_data/train/dia1utt8.mp4']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "\"Your task is an emotion classification and cause recognition. \"\n",
    "\"Predict the emotion of the speaker in each utterance and the ids of \"\n",
    "\"utterances that caused the speaker's emotion in each utterance, then \"\n",
    "\"place the target utterances and its emotions as sorted keys and a sorted list of ids of \"\n",
    "\"causal utterances as values in an output JSON payload. Below is an example: \"\n",
    "'Input: Rachel: \"Mom, would you relax.\", Rachel: \"That was 10 blocks from here and, '\n",
    "'the, the woman was walking alone at night, I would never do that.\", Rachel: \"Mom, c mon, '\n",
    "'stop worrying.\". Output: {\"1_neutral\": [], \"2_sadness\": [3], \"3_sadness\": [3]}. Now, complete the task. Input: '\n",
    "input_text\n",
    "\" Output:\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
