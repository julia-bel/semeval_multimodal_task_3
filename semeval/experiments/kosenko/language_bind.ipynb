{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/code/semeval/experiments/kosenko/LanguageBind/languagebind/audio/processing_audio.py:17: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "\n",
    "\n",
    "from semeval.experiments.kosenko.LanguageBind.languagebind import (\n",
    "    LanguageBind,\n",
    "\n",
    "    to_device,\n",
    "    transform_dict,\n",
    "    LanguageBindImageTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "device = torch.device(device)\n",
    "clip_type = {\n",
    "    \"video\": \"LanguageBind_Video_FT\",  # also LanguageBind_Video\n",
    "    \"audio\": \"LanguageBind_Audio_FT\",  # also LanguageBind_Audio\n",
    "    \"thermal\": \"LanguageBind_Thermal\",\n",
    "    \"image\": \"LanguageBind_Image\",\n",
    "    \"depth\": \"LanguageBind_Depth\",\n",
    "}\n",
    "\n",
    "model = LanguageBind(clip_type=clip_type, cache_dir=\"/code/cache_dir\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "pretrained_ckpt = f\"LanguageBind/LanguageBind_Image\"\n",
    "tokenizer = LanguageBindImageTokenizer.from_pretrained(\n",
    "    pretrained_ckpt, cache_dir=\"/code/cache_dir/tokenizer_cache_dir\"\n",
    ")\n",
    "modality_transform = {\n",
    "    c: transform_dict[c](model.modality_config[c]) for c in clip_type.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video x Text: \n",
      " tensor([[9.2179e-08, 6.5961e-01, 6.4906e-04, 3.3974e-01, 5.2970e-09]],\n",
      "       device='cuda:0')\n",
      "Most similar text is: \n",
      "Training a parrot to climb up a ladder.\n",
      "--\n",
      "Parrot climbs the small stairs.\n",
      "--\n",
      "Cute parrot is sitting on the floor.\n",
      "--\n",
      "A lion climbing a tree to catch a monkey.\n",
      "--\n",
      "Two pandas are eating bamboo.\n"
     ]
    }
   ],
   "source": [
    "video = [\n",
    "    \"semeval/experiments/kosenko/LanguageBind/assets/video/0.mp4\",\n",
    "]\n",
    "language = [\n",
    "    \"A lion climbing a tree to catch a monkey.\",\n",
    "    \"Training a parrot to climb up a ladder.\",\n",
    "    \"Cute parrot is sitting on the floor.\",\n",
    "    \"Parrot climbs the small stairs.\",\n",
    "    \"Two pandas are eating bamboo.\",\n",
    "]\n",
    "\n",
    "inputs = {\n",
    "    \"video\": to_device(modality_transform[\"video\"](video), device),\n",
    "}\n",
    "inputs[\"language\"] = to_device(\n",
    "    tokenizer(\n",
    "        language,\n",
    "        max_length=77,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ),\n",
    "    device,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "probs = torch.softmax(embeddings[\"video\"] @ embeddings[\"language\"].T, dim=-1)\n",
    "sorted_indices = probs.topk(k=len(language)).indices.squeeze().tolist()\n",
    "predicted_texts = \"\\n--\\n\".join([language[pos] for pos in sorted_indices])\n",
    "\n",
    "print(\"Video x Text: \\n\", probs)\n",
    "print(f\"Most similar text is: \\n{predicted_texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semeval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_ID': 1,\n",
       " 'conversation': [{'utterance_ID': 1,\n",
       "   'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia1utt1.mp4'},\n",
       "  {'utterance_ID': 2,\n",
       "   'text': 'Oh , yeah . Had that dream .',\n",
       "   'speaker': 'All',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia1utt2.mp4'},\n",
       "  {'utterance_ID': 3,\n",
       "   'text': 'Then I look down , and I realize there is a phone ... there .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'surprise',\n",
       "   'video_name': 'dia1utt3.mp4'},\n",
       "  {'utterance_ID': 4,\n",
       "   'text': 'Instead of ... ?',\n",
       "   'speaker': 'Joey',\n",
       "   'emotion': 'surprise',\n",
       "   'video_name': 'dia1utt4.mp4'},\n",
       "  {'utterance_ID': 5,\n",
       "   'text': 'That is right .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'anger',\n",
       "   'video_name': 'dia1utt5.mp4'},\n",
       "  {'utterance_ID': 6,\n",
       "   'text': 'Never had that dream .',\n",
       "   'speaker': 'Joey',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia1utt6.mp4'},\n",
       "  {'utterance_ID': 7,\n",
       "   'text': 'No .',\n",
       "   'speaker': 'Phoebe',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia1utt7.mp4'},\n",
       "  {'utterance_ID': 8,\n",
       "   'text': 'All of a sudden , the phone starts to ring .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'neutral',\n",
       "   'video_name': 'dia1utt8.mp4'}],\n",
       " 'emotion-cause_pairs': [['3_surprise', '1'],\n",
       "  ['3_surprise', '3'],\n",
       "  ['4_surprise', '1'],\n",
       "  ['4_surprise', '3'],\n",
       "  ['4_surprise', '4'],\n",
       "  ['5_anger', '1'],\n",
       "  ['5_anger', '3'],\n",
       "  ['5_anger', '4']]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dataset = json.loads(\n",
    "    open(\"./SemEval-2024_Task3/training_data/Subtask_2_train.json\").read()\n",
    ")\n",
    "conversation = dataset[0]\n",
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: neutral \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: anger \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: anger \n",
      "True Emotion: surprise\n",
      "---\n",
      "Predicted emotion: anger \n",
      "True Emotion: surprise\n",
      "---\n",
      "Predicted emotion: anger \n",
      "True Emotion: anger\n",
      "---\n",
      "Predicted emotion: anger \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: anger \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: surprise \n",
      "True Emotion: neutral\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# emotions = list(set([item[\"emotion\"] for item in conversation[\"conversation\"]]))\n",
    "emotions = list(set([f'{item[\"emotion\"]}' for item in conversation[\"conversation\"]]))\n",
    "\n",
    "\n",
    "for dialogue_turn in conversation[\"conversation\"]:\n",
    "    video_path = dialogue_turn[\"video_name\"]\n",
    "    video = [\n",
    "        f\"SemEval-2024_Task3/training_data/train/{video_path}\",\n",
    "    ]\n",
    "\n",
    "    inputs = {\n",
    "        \"video\": to_device(modality_transform[\"video\"](video), device),\n",
    "    }\n",
    "    inputs[\"language\"] = to_device(\n",
    "        tokenizer(\n",
    "            emotions,\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ),\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "    probs = torch.softmax(embeddings[\"video\"] @ embeddings[\"language\"].T, dim=-1)\n",
    "    sorted_indices = probs.topk(k=len(emotions)).indices.squeeze().tolist()[0]\n",
    "    print(f\"Predicted emotion: {emotions[sorted_indices]} \")\n",
    "    print(f\"True Emotion: {dialogue_turn['emotion']}\")\n",
    "    print(\"---\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SemEval-2024_Task3/training_data/train/dia2utt3.mp4']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract audio from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/18756 [00:00<08:04, 38.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18756/18756 [10:35<00:00, 29.50it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "video_names = os.listdir(\"SemEval-2024_Task3/training_data/train\")\n",
    "for video_name in tqdm.tqdm(video_names):\n",
    "    base_folder = \"SemEval-2024_Task3/training_data/train/\"\n",
    "    video_path = f\"{base_folder}{video_name}\"\n",
    "    audio_path = video_path.replace(\".mp4\", \".wav\")\n",
    "    os.system(\n",
    "        f\"ffmpeg -loglevel quiet -i {video_path} -vn -acodec pcm_s16le -ar 44100 -ac 2 {audio_path} -y\"\n",
    "    )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: neutral emotion replica: Oh my God ! \n",
      "True Emotion: surprise\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: I know , I know , I am such an idiot . \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: I guess I should have caught on when she started going to the dentist four and five times a week . I mean , how clean can teeth get ? \n",
      "True Emotion: sadness\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: My brother going through that right now , he is such a mess . How did you get through it ? \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: Well , you might try accidentally breaking something valuable of hers , say her ... \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: leg ? \n",
      "True Emotion: surprise\n",
      "---\n",
      "Predicted emotion: surprise emotion replica: That is one way ! \n",
      "True Emotion: joy\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: Me , I ... I went for the watch . \n",
      "True Emotion: neutral\n",
      "---\n",
      "Predicted emotion: neutral emotion replica: You actually broke her watch ? \n",
      "True Emotion: surprise\n",
      "---\n",
      "Accuracy: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "conversation = dataset[2]\n",
    "# emotions = list(set([item[\"emotion\"] for item in conversation[\"conversation\"]]))\n",
    "emotions = list(\n",
    "    set([f'{item[\"emotion\"]} emotion replica' for item in conversation[\"conversation\"]])\n",
    ")\n",
    "\n",
    "correct_answers = 0\n",
    "for dialogue_turn in conversation[\"conversation\"]:\n",
    "    video_path = dialogue_turn[\"video_name\"]\n",
    "    audio_path = video_path.replace(\".mp4\", \".wav\")\n",
    "    audio = [\n",
    "        f\"SemEval-2024_Task3/training_data/train/{audio_path}\",\n",
    "    ]\n",
    "\n",
    "    inputs = {\n",
    "        # \"video\": to_device(modality_transform[\"video\"](video), device),\n",
    "        \"audio\": to_device(modality_transform[\"audio\"](audio), device),\n",
    "    }\n",
    "    dialogue_emotions = [f'{emotion}: {dialogue_turn[\"text\"]}' for emotion in emotions]\n",
    "    inputs[\"language\"] = to_device(\n",
    "        tokenizer(\n",
    "            dialogue_emotions,\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ),\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "    probs = torch.softmax(embeddings[\"audio\"] @ embeddings[\"language\"].T, dim=-1)\n",
    "    sorted_indices = probs.topk(k=len(emotions)).indices.squeeze().tolist()[0]\n",
    "    predicted_emotion = dialogue_emotions[sorted_indices]\n",
    "    true_emotion = dialogue_turn[\"emotion\"]\n",
    "    if true_emotion in predicted_emotion:\n",
    "        correct_answers += 1\n",
    "    print(f\"Predicted emotion: {predicted_emotion} \")\n",
    "    print(f\"True Emotion: {true_emotion}\")\n",
    "    print(\"---\")\n",
    "    # break\n",
    "print(f\"Accuracy: {correct_answers/len(conversation['conversation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [13:15<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.08774463087320328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "total_accurary = []\n",
    "emotions = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "\n",
    "for conversation in tqdm.tqdm(dataset):\n",
    "    # conversation = dataset[2]\n",
    "    # emotions = list(set([item[\"emotion\"] for item in conversation[\"conversation\"]]))\n",
    "\n",
    "    correct_answers = 0\n",
    "    for dialogue_turn in conversation[\"conversation\"]:\n",
    "        video_path = dialogue_turn[\"video_name\"]\n",
    "        audio_path = video_path.replace(\".mp4\", \".wav\")\n",
    "        audio = [\n",
    "            f\"SemEval-2024_Task3/training_data/train/{audio_path}\",\n",
    "        ]\n",
    "\n",
    "        inputs = {\n",
    "            # \"video\": to_device(modality_transform[\"video\"](video), device),\n",
    "            \"audio\": to_device(modality_transform[\"audio\"](audio), device),\n",
    "        }\n",
    "        # dialogue_emotions = [\n",
    "        #     f'{emotion}: {dialogue_turn[\"text\"]}' for emotion in emotions\n",
    "        # ]\n",
    "        dialogue_emotions = emotions\n",
    "        inputs[\"language\"] = to_device(\n",
    "            tokenizer(\n",
    "                dialogue_emotions,\n",
    "                max_length=77,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ),\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(inputs)\n",
    "\n",
    "        probs = torch.softmax(embeddings[\"audio\"] @ embeddings[\"language\"].T, dim=-1)\n",
    "        # sorted_indices = probs.topk(k=len(emotions)).indices.squeeze().tolist()[0]\n",
    "        sorted_indices = int(probs.argmax())\n",
    "        predicted_emotion = dialogue_emotions[sorted_indices]\n",
    "        true_emotion = dialogue_turn[\"emotion\"]\n",
    "        if true_emotion in predicted_emotion:\n",
    "            correct_answers += 1\n",
    "        # print(f\"Predicted emotion: {predicted_emotion} \")\n",
    "        # print(f\"True Emotion: {true_emotion}\")\n",
    "        # print(\"---\")\n",
    "        # break\n",
    "    local_accuracy = correct_answers / len(conversation[\"conversation\"])\n",
    "    total_accurary.append(local_accuracy)\n",
    "    # print(f\"Accuracy: {local_accuracy}\")\n",
    "    # break\n",
    "\n",
    "print(f\"Total accuracy: {torch.mean(torch.tensor(total_accurary))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
