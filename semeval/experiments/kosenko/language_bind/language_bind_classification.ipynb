{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/code/semeval/experiments/kosenko/language_bind/LanguageBind/languagebind/audio/processing_audio.py:17: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "\n",
    "\n",
    "from semeval.experiments.kosenko.language_bind.LanguageBind.languagebind import (\n",
    "    LanguageBind,\n",
    "    to_device,\n",
    "    transform_dict,\n",
    "    LanguageBindImageTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "device = torch.device(device)\n",
    "clip_type = {\n",
    "    \"video\": \"LanguageBind_Video_FT\",  # also LanguageBind_Video\n",
    "    # \"audio\": \"LanguageBind_Audio_FT\",  # also LanguageBind_Audio\n",
    "    # \"thermal\": \"LanguageBind_Thermal\",\n",
    "    # \"image\": \"LanguageBind_Image\",\n",
    "    # \"depth\": \"LanguageBind_Depth\",\n",
    "}\n",
    "\n",
    "languagebind_model = LanguageBind(clip_type=clip_type, cache_dir=\"/code/cache_dir\")\n",
    "languagebind_model = languagebind_model.to(device)\n",
    "# model.eval()\n",
    "pretrained_ckpt = f\"LanguageBind/LanguageBind_Image\"\n",
    "tokenizer = LanguageBindImageTokenizer.from_pretrained(\n",
    "    pretrained_ckpt, cache_dir=\"/code/cache_dir/tokenizer_cache_dir\"\n",
    ")\n",
    "modality_transform = {\n",
    "    c: transform_dict[c](languagebind_model.modality_config[c])\n",
    "    for c in clip_type.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "video = [\n",
    "    \"semeval/experiments/kosenko/language_bind/LanguageBind/assets/video/0.mp4\",\n",
    "    \"semeval/experiments/kosenko/language_bind/LanguageBind/assets/video/0.mp4\",\n",
    "]\n",
    "language = [\n",
    "    \"Two pandas are eating bamboo.\",\n",
    "    \"Two pandas are eating bamboo.\",\n",
    "]\n",
    "\n",
    "inputs = {\n",
    "    \"video\": to_device(modality_transform[\"video\"](video), device),\n",
    "}\n",
    "inputs[\"language\"] = to_device(\n",
    "    tokenizer(\n",
    "        language,\n",
    "        max_length=77,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ),\n",
    "    device,\n",
    ")\n",
    "\n",
    "\n",
    "class VideoTextClassif(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = languagebind_model\n",
    "        self.linear = torch.nn.Linear(768 * 2, 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x)\n",
    "        # print(result)\n",
    "        features = torch.cat(\n",
    "            [\n",
    "                result[\"video\"],\n",
    "                result[\"language\"],\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        result = self.linear(features)\n",
    "        return result\n",
    "\n",
    "\n",
    "text_video_classif = VideoTextClassif()\n",
    "text_video_classif.to(device)\n",
    "output = text_video_classif(inputs)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_func(output, torch.tensor([1, 0], device=device))\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент 1\n",
    "\n",
    "Классификатор на основе текста и видео. На вход подается независимая реплика диалога и соответствующее видео к нему. Никакой следующий контекст диалога не используется.\n",
    "\n",
    "На основе этого нужно предсказать эмоцию, то есть класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374\n",
      "13619\n",
      "{'surprise': 0, 'fear': 1, 'sadness': 2, 'neutral': 3, 'joy': 4, 'anger': 5, 'disgust': 6}\n",
      "{0: 'surprise', 1: 'fear', 2: 'sadness', 3: 'neutral', 4: 'joy', 5: 'anger', 6: 'disgust'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'utterance_ID': tensor([ 2, 13]),\n",
       " 'text': ['Oh , I can not tell you how great it was to look at the crowd and see your face !',\n",
       "  'Hey .'],\n",
       " 'speaker': ['Ross', 'Rachel'],\n",
       " 'emotion': ['joy', 'neutral'],\n",
       " 'video_name': ['/code/SemEval-2024_Task3/training_data/train/dia1368utt2.mp4',\n",
       "  '/code/SemEval-2024_Task3/training_data/train/dia147utt13.mp4'],\n",
       " 'label': tensor([4, 3])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def random_seed(seed=42, rank=0):\n",
    "    torch.manual_seed(seed + rank)\n",
    "    np.random.seed(seed + rank)\n",
    "    random.seed(seed + rank)\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchvision.io import read_video\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "dataset_path = \"./SemEval-2024_Task3/training_data/Subtask_2_train.json\"\n",
    "\n",
    "\n",
    "dataset = json.loads(open(dataset_path).read())\n",
    "print(len(dataset))\n",
    "\n",
    "\n",
    "# dataset[0]\n",
    "\n",
    "\n",
    "all_conversations = []\n",
    "\n",
    "\n",
    "for item in dataset:\n",
    "    all_conversations.extend(item[\"conversation\"])\n",
    "print(len(all_conversations))\n",
    "\n",
    "\n",
    "# all_emotions = set([])\n",
    "\n",
    "\n",
    "# for item in all_conversations:\n",
    "\n",
    "\n",
    "#     all_emotions.update([item[\"emotion\"]])\n",
    "# for item in all_conversations:\n",
    "#     print(item['video_name'])\n",
    "\n",
    "\n",
    "# print(all_emotions)\n",
    "\n",
    "\n",
    "all_emotions = [\n",
    "    \"surprise\",\n",
    "    \"fear\",\n",
    "    \"sadness\",\n",
    "    \"neutral\",\n",
    "    \"joy\",\n",
    "    \"anger\",\n",
    "    \"disgust\",\n",
    "]\n",
    "\n",
    "\n",
    "emotions2labels = {em: i for i, em in enumerate(all_emotions)}\n",
    "\n",
    "\n",
    "labels2emotions = {i: em for i, em in enumerate(all_emotions)}\n",
    "\n",
    "\n",
    "print(emotions2labels)\n",
    "\n",
    "\n",
    "print(labels2emotions)\n",
    "\n",
    "\n",
    "training_data, test_data = train_test_split(all_conversations, test_size=0.04)\n",
    "\n",
    "\n",
    "class ConversationsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conversations,\n",
    "        base_video_path=\"/code/SemEval-2024_Task3/training_data/train\",\n",
    "    ):\n",
    "        self.conversations = conversations\n",
    "\n",
    "        self.base_video_path = base_video_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        turn = self.conversations[idx]\n",
    "\n",
    "        video_path = turn[\"video_name\"]\n",
    "\n",
    "        turn[\"video_name\"] = f\"{self.base_video_path}/{video_path}\"\n",
    "\n",
    "        turn[\"label\"] = emotions2labels[turn[\"emotion\"]]\n",
    "\n",
    "        return turn\n",
    "\n",
    "\n",
    "training_data = ConversationsDataset(conversations=training_data)\n",
    "test_data = ConversationsDataset(conversations=test_data)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13619, 13074, 545)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_conversations), len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utterance_ID', 'text', 'speaker', 'emotion', 'video_name', 'label']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(next(iter(train_dataloader)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/code/semeval/experiments/kosenko/language_bind/LanguageBind/languagebind/audio/processing_audio.py:17: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "from semeval.experiments.kosenko.language_bind.LanguageBind.languagebind import (\n",
    "    LanguageBind,\n",
    "    to_device,\n",
    "    transform_dict,\n",
    "    LanguageBindImageTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "class VideoTextClassif(torch.nn.Module):\n",
    "    def __init__(self, labels=2, clip_type=None):\n",
    "        super().__init__()\n",
    "        self.model = LanguageBind(\n",
    "            clip_type=clip_type,\n",
    "            cache_dir=\"/code/cache_dir\",\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(\n",
    "            768 * 2,\n",
    "            labels,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x)\n",
    "        # print(result)\n",
    "        features = torch.cat(\n",
    "            [\n",
    "                result[\"video\"],\n",
    "                result[\"language\"],\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        result = self.linear(features)\n",
    "        return result\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "device = torch.device(device)\n",
    "clip_type = {\n",
    "    \"video\": \"LanguageBind_Video_FT\",\n",
    "}\n",
    "text_video_classif = VideoTextClassif(\n",
    "    labels=len(all_emotions),\n",
    "    clip_type=clip_type,\n",
    ")\n",
    "text_video_classif = text_video_classif.to(device)\n",
    "pretrained_ckpt = f\"LanguageBind/LanguageBind_Image\"\n",
    "tokenizer = LanguageBindImageTokenizer.from_pretrained(\n",
    "    pretrained_ckpt, cache_dir=\"/code/cache_dir/tokenizer_cache_dir\"\n",
    ")\n",
    "modality_transform = {\n",
    "    c: transform_dict[c](text_video_classif.model.modality_config[c])\n",
    "    for c in clip_type.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "text_video_classif.eval()\n",
    "batch = next(iter(train_dataloader))\n",
    "inputs = {\n",
    "    \"video\": to_device(\n",
    "        modality_transform[\"video\"](batch[\"video_name\"]),\n",
    "        device,\n",
    "    ),\n",
    "    \"language\": to_device(\n",
    "        tokenizer(\n",
    "            batch[\"text\"],\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ),\n",
    "        device,\n",
    "    ),\n",
    "}\n",
    "\n",
    "result = text_video_classif(inputs)\n",
    "predicted_labels = result.argmax(-1).cpu().numpy()\n",
    "test_f1_score = f1_score(\n",
    "    batch[\"label\"].numpy(),\n",
    "    predicted_labels,\n",
    "    average=\"macro\",\n",
    ")\n",
    "print(test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6992, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    text_video_classif.parameters(),\n",
    "    lr=0.00001,\n",
    ")\n",
    "loss = loss_func(\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0.45, 0.23],\n",
    "            [0.55, 0.33],\n",
    "        ],\n",
    "        device=device,\n",
    "    ),\n",
    "    torch.tensor([1, 0], device=device),\n",
    ")\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### super simple train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.4116463661193848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.118516206741333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.8778287172317505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.1899290084838867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.954799175262451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.45052316784858704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.4964609742164612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:05,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.371068000793457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:06,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.45426976680755615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:07,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1.9046975374221802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.7073445916175842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:09,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1.973649263381958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:09,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.9785594344139099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:10,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.038552761077881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:11,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1.4154369831085205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 3.287311315536499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:13,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 3.294203042984009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:13,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 3.5777933597564697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:14,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.9182525873184204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:15,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 1.3086097240447998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:16,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2.188847541809082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:17,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 2.370687484741211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:17,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 2.256296396255493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:18,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 1.4581425189971924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:19,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 1.0644042491912842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:20,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 1.9248542785644531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:20,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 1.8852068185806274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:21,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 1.6872577667236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:22,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 0.965337872505188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:23,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 1.7070598602294922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:23,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 3.010525941848755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:24,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 2.3659281730651855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:25,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.45751649141311646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:26,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 0.5569820404052734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:26,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 0.28989309072494507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:27,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 2.116980791091919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:28,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 2.9667627811431885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:29,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 2.1935653686523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:30,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 3.322813034057617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:31,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 5.079277992248535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:31,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 1.2822527885437012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:32,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 0.32359644770622253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:33,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 3.404543399810791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:34,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 2.111872673034668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:35,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 1.7865755558013916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:36,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 1.4486266374588013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:36,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 1.4201889038085938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:37,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 1.8882529735565186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:38,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 1.2243683338165283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:39,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 1.8630074262619019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:39,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 2.4275169372558594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:40,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 2.320770740509033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:41,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 1.6899826526641846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [00:42,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 2.2412166595458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:43,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 2.807105541229248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:43,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 2.1958789825439453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [00:44,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 2.089587688446045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:45,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 2.406179189682007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:46,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 1.3997547626495361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:47,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 1.6681780815124512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:47,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 1.758849859237671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:48,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 2.2648091316223145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:49,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 1.2803257703781128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:50,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 1.9710869789123535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:50,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 3.0484325885772705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [00:51,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 2.3213601112365723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:52,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 2.213437080383301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [00:53,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 2.856510639190674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [00:53,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 1.6671693325042725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:54,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 2.405856132507324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:55,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 3.700026512145996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:56,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 1.2051950693130493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:56,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 1.890251636505127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [00:57,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 1.8185186386108398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [00:58,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 1.4375560283660889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:59,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 2.1991066932678223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [01:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 1.668765664100647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [01:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 1.2664744853973389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [01:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 1.3477237224578857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:02,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 1.5215569734573364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:03,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 2.007652521133423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [01:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 2.2828333377838135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [01:05,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 0.8409034013748169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [01:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 1.5698870420455933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [01:06,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 3.00311279296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [01:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 1.9673261642456055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [01:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 0.7008718252182007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [01:08,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 2.5872793197631836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [01:09,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 3.4001688957214355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [01:10,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 3.4822511672973633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [01:11,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 3.230837345123291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [01:12,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 1.3050684928894043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:12,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 1.7404236793518066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [01:13,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 2.41038179397583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [01:14,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 1.0576368570327759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [01:15,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 1.3588182926177979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [01:16,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 1.4618091583251953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [01:16,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 1.3760355710983276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [01:17,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 1.1598066091537476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [01:18,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.3700892925262451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:19,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2.2770862579345703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:19,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 1.2192869186401367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:20,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "max_train_steps = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for num_step, batch in tqdm.tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch)\n",
    "        inputs = {\n",
    "            \"video\": to_device(\n",
    "                modality_transform[\"video\"](batch[\"video_name\"]), device\n",
    "            ),\n",
    "            \"language\": to_device(\n",
    "                tokenizer(\n",
    "                    batch[\"text\"],\n",
    "                    max_length=77,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ),\n",
    "                device,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        result = text_video_classif(inputs)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        loss = loss_func(result, label)\n",
    "        print(num_step, loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if num_step > max_train_steps:\n",
    "            break\n",
    "\n",
    "    # for\n",
    "    #     # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### huggingface trainer (падает по памяти)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([5, 5], device='cuda:0'), 'utterance_ID': tensor([5, 5], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'video_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 89\u001b[0m\n\u001b[1;32m     80\u001b[0m hf_test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list([item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[1;32m     82\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     83\u001b[0m     model\u001b[38;5;241m=\u001b[39mtext_video_classif,\n\u001b[1;32m     84\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     88\u001b[0m )\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2759\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2758\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2759\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2762\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     12\u001b[0m     model,\n\u001b[1;32m     13\u001b[0m     inputs,\n\u001b[1;32m     14\u001b[0m     return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m ):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[1;32m     17\u001b[0m     custom_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m: to_device(\n\u001b[0;32m---> 19\u001b[0m             modality_transform[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m](\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[1;32m     20\u001b[0m             device,\n\u001b[1;32m     21\u001b[0m         ),\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: to_device(\n\u001b[1;32m     23\u001b[0m             tokenizer(\n\u001b[1;32m     24\u001b[0m                 inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m                 max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m77\u001b[39m,\n\u001b[1;32m     26\u001b[0m                 padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m                 truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m                 return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m             ),\n\u001b[1;32m     30\u001b[0m             device,\n\u001b[1;32m     31\u001b[0m         ),\n\u001b[1;32m     32\u001b[0m     }\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(custom_inputs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'video_name'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# import torch\n",
    "# from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "# from semeval.experiments.kosenko.language_bind.LanguageBind.languagebind import (\n",
    "#     LanguageBind,\n",
    "#     to_device,\n",
    "#     transform_dict,\n",
    "#     LanguageBindImageTokenizer,\n",
    "# )\n",
    "# from typing import Dict, List, Optional\n",
    "# from torch import nn\n",
    "# from torch.utils.data import Dataset\n",
    "# from transformers import Trainer\n",
    "# from transformers import TrainingArguments\n",
    "# import datasets\n",
    "\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "\n",
    "# def random_seed(seed=42, rank=0):\n",
    "#     torch.manual_seed(seed + rank)\n",
    "#     np.random.seed(seed + rank)\n",
    "#     random.seed(seed + rank)\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from torchvision.io import read_video\n",
    "# import json\n",
    "# import torch\n",
    "# import os\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(\n",
    "#         self,\n",
    "#         model,\n",
    "#         inputs,\n",
    "#         return_outputs=False,\n",
    "#     ):\n",
    "#         video_paths = [\n",
    "#             f\"{base_path}/{video_path}\"\n",
    "#             for base_path, video_path in zip(\n",
    "#                 inputs[\"video_base_path\"], inputs[\"video_name\"]\n",
    "#             )\n",
    "#         ]\n",
    "#         custom_inputs = {\n",
    "#             \"video\": to_device(\n",
    "#                 modality_transform[\"video\"](video_paths),\n",
    "#                 device,\n",
    "#             ),\n",
    "#             \"language\": to_device(\n",
    "#                 tokenizer(\n",
    "#                     inputs[\"text\"],\n",
    "#                     max_length=77,\n",
    "#                     padding=\"max_length\",\n",
    "#                     truncation=True,\n",
    "#                     return_tensors=\"pt\",\n",
    "#                 ),\n",
    "#                 device,\n",
    "#             ),\n",
    "#         }\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(custom_inputs)\n",
    "#         label = inputs[\"label\"].to(device)\n",
    "#         loss_func = torch.nn.CrossEntropyLoss()\n",
    "#         loss = loss_func(outputs, label)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "#     def get_train_dataloader(self):\n",
    "#         train_dataset = self.train_dataset\n",
    "#         return DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=self.args.per_device_train_batch_size,\n",
    "#             shuffle=True,\n",
    "#         )\n",
    "\n",
    "#     def get_eval_dataloader(self, eval_dataset):\n",
    "#         return DataLoader(\n",
    "#             self.eval_dataset,\n",
    "#             batch_size=self.args.per_device_eval_batch_size,\n",
    "#             shuffle=False,\n",
    "#         )\n",
    "\n",
    "#     def prediction_step(\n",
    "#         self,\n",
    "#         model,\n",
    "#         inputs,\n",
    "#         prediction_loss_only,\n",
    "#         ignore_keys=None,\n",
    "#     ):\n",
    "#         # print(inputs)\n",
    "#         video_paths = [\n",
    "#             f\"{base_path}/{video_path}\"\n",
    "#             for base_path, video_path in zip(\n",
    "#                 inputs[\"video_base_path\"], inputs[\"video_name\"]\n",
    "#             )\n",
    "#         ]\n",
    "#         custom_inputs = {\n",
    "#             \"video\": to_device(\n",
    "#                 modality_transform[\"video\"](video_paths),\n",
    "#                 device,\n",
    "#             ),\n",
    "#             \"language\": to_device(\n",
    "#                 tokenizer(\n",
    "#                     inputs[\"text\"],\n",
    "#                     max_length=77,\n",
    "#                     padding=\"max_length\",\n",
    "#                     truncation=True,\n",
    "#                     return_tensors=\"pt\",\n",
    "#                 ),\n",
    "#                 device,\n",
    "#             ),\n",
    "#         }\n",
    "\n",
    "#         # forward pass\n",
    "#         outputs = model(custom_inputs)\n",
    "#         label = inputs[\"label\"].to(device)\n",
    "#         loss_func = torch.nn.CrossEntropyLoss()\n",
    "#         loss = loss_func(outputs, label)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         if prediction_loss_only:\n",
    "#             return (loss, None, None)\n",
    "\n",
    "#         return (loss, outputs, label)\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     # metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "#     print(eval_preds)\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     # return metric.compute(predictions=predictions, references=labels)\n",
    "#     return 1.0\n",
    "\n",
    "\n",
    "# class VideoTextClassif(torch.nn.Module):\n",
    "#     def __init__(self, labels=2, clip_type=None):\n",
    "#         super().__init__()\n",
    "#         self.model = LanguageBind(\n",
    "#             clip_type=clip_type,\n",
    "#             cache_dir=\"/code/cache_dir\",\n",
    "#         )\n",
    "#         self.linear = torch.nn.Linear(\n",
    "#             768 * 2,\n",
    "#             labels,\n",
    "#             bias=False,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         result = self.model(x)\n",
    "#         # print(result)\n",
    "#         features = torch.cat(\n",
    "#             [\n",
    "#                 result[\"video\"],\n",
    "#                 result[\"language\"],\n",
    "#             ],\n",
    "#             dim=-1,\n",
    "#         )\n",
    "#         result = self.linear(features)\n",
    "#         return result\n",
    "\n",
    "\n",
    "# class ConversationsDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         conversations,\n",
    "#         base_video_path=\"/code/SemEval-2024_Task3/training_data/train\",\n",
    "#     ):\n",
    "#         self.conversations = conversations\n",
    "\n",
    "#         self.base_video_path = base_video_path\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.conversations)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         turn = self.conversations[idx]\n",
    "\n",
    "#         turn[\"video_name\"] = turn[\"video_name\"]\n",
    "#         turn[\"video_base_path\"] = self.base_video_path\n",
    "#         # print(video_path)\n",
    "#         turn[\"label\"] = emotions2labels[turn[\"emotion\"]]\n",
    "\n",
    "#         return turn\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     dataset_path = \"./SemEval-2024_Task3/training_data/Subtask_2_train.json\"\n",
    "\n",
    "#     dataset = json.loads(open(dataset_path).read())\n",
    "#     print(len(dataset))\n",
    "\n",
    "#     # dataset[0]\n",
    "\n",
    "#     all_conversations = []\n",
    "\n",
    "#     for item in dataset:\n",
    "#         all_conversations.extend(item[\"conversation\"])\n",
    "#     print(len(all_conversations))\n",
    "\n",
    "#     all_emotions = [\n",
    "#         \"surprise\",\n",
    "#         \"fear\",\n",
    "#         \"sadness\",\n",
    "#         \"neutral\",\n",
    "#         \"joy\",\n",
    "#         \"anger\",\n",
    "#         \"disgust\",\n",
    "#     ]\n",
    "\n",
    "#     emotions2labels = {em: i for i, em in enumerate(all_emotions)}\n",
    "\n",
    "#     labels2emotions = {i: em for i, em in enumerate(all_emotions)}\n",
    "\n",
    "#     print(emotions2labels)\n",
    "\n",
    "#     print(labels2emotions)\n",
    "\n",
    "#     training_data_list, test_data_list = train_test_split(\n",
    "#         all_conversations, test_size=0.04\n",
    "#     )\n",
    "#     training_data_list = training_data_list[:10]\n",
    "#     test_data_list = test_data_list[:10]\n",
    "#     training_data = ConversationsDataset(conversations=training_data_list)\n",
    "#     test_data = ConversationsDataset(conversations=test_data_list)\n",
    "\n",
    "#     # train_dataloader = DataLoader(training_data, batch_size=2, shuffle=True)\n",
    "#     # test_dataloader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
    "\n",
    "#     # next(iter(train_dataloader))\n",
    "#     device = \"cuda:0\"\n",
    "#     device = torch.device(device)\n",
    "#     clip_type = {\n",
    "#         \"video\": \"LanguageBind_Video_FT\",\n",
    "#     }\n",
    "#     text_video_classif = VideoTextClassif(\n",
    "#         labels=len(all_emotions),\n",
    "#         clip_type=clip_type,\n",
    "#     )\n",
    "#     text_video_classif = text_video_classif.to(device)\n",
    "#     # text_video_classif.half()\n",
    "#     pretrained_ckpt = f\"LanguageBind/LanguageBind_Image\"\n",
    "#     tokenizer = LanguageBindImageTokenizer.from_pretrained(\n",
    "#         pretrained_ckpt, cache_dir=\"/code/cache_dir/tokenizer_cache_dir\"\n",
    "#     )\n",
    "#     modality_transform = {\n",
    "#         c: transform_dict[c](text_video_classif.model.modality_config[c])\n",
    "#         for c in clip_type.keys()\n",
    "#     }\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"semeval/experiments/kosenko/language_bind/train_results/\",\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         eval_steps=1,\n",
    "#         report_to=\"none\",\n",
    "#         per_device_train_batch_size=1,\n",
    "#         per_device_eval_batch_size=1,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         fp16=True,\n",
    "#         remove_unused_columns=False,\n",
    "#         label_names=[\n",
    "#             \"utterance_ID\",\n",
    "#             \"text\",\n",
    "#             \"speaker\",\n",
    "#             \"emotion\",\n",
    "#             \"video_name\",\n",
    "#             \"label\",\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "#     # hf_training_data = datasets.Dataset.from_list([item for item in training_data])\n",
    "#     # hf_test_data = datasets.Dataset.from_list([item for item in test_data])\n",
    "\n",
    "#     trainer = CustomTrainer(\n",
    "#         model=text_video_classif,\n",
    "#         args=training_args,\n",
    "#         # train_dataset=hf_training_data,\n",
    "#         # eval_dataset=hf_test_data,\n",
    "#         train_dataset=training_data,\n",
    "#         eval_dataset=test_data,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utterance_ID', 'text', 'speaker', 'emotion', 'video_name', 'label']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_training_data.column_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
