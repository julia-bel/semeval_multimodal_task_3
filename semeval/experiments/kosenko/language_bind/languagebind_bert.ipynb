{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/code/semeval/experiments/kosenko/language_bind/LanguageBind/languagebind/audio/processing_audio.py:17: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video x Text: \n",
      " tensor([[1.3242e-07, 6.8035e-01, 1.0337e-03, 3.1862e-01, 3.9172e-09]],\n",
      "       device='cuda:0')\n",
      "Most similar text is: \n",
      "Training a parrot to climb up a ladder.\n",
      "--\n",
      "Parrot climbs the small stairs.\n",
      "--\n",
      "Cute parrot is sitting on the floor.\n",
      "--\n",
      "A lion climbing a tree to catch a monkey.\n",
      "--\n",
      "Two pandas are eating bamboo.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "from semeval.experiments.kosenko.language_bind.LanguageBind.languagebind import (\n",
    "    LanguageBind,\n",
    "    to_device,\n",
    "    transform_dict,\n",
    "    LanguageBindImageTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "all_emotions = [\n",
    "    \"surprise\",\n",
    "    \"fear\",\n",
    "    \"sadness\",\n",
    "    \"neutral\",\n",
    "    \"joy\",\n",
    "    \"anger\",\n",
    "    \"disgust\",\n",
    "]\n",
    "\n",
    "emotions2labels = {em: i for i, em in enumerate(all_emotions)}\n",
    "\n",
    "labels2emotions = {i: em for i, em in enumerate(all_emotions)}\n",
    "\n",
    "clip_type = {\n",
    "    \"video\": \"LanguageBind_Video_FT\",\n",
    "}\n",
    "\n",
    "model = LanguageBind(\n",
    "    clip_type=clip_type,\n",
    "    cache_dir=\"/code/cache_dir\",\n",
    ")\n",
    "pretrained_ckpt = f\"LanguageBind/LanguageBind_Image\"\n",
    "tokenizer = LanguageBindImageTokenizer.from_pretrained(\n",
    "    pretrained_ckpt, cache_dir=\"/code/cache_dir/tokenizer_cache_dir\"\n",
    ")\n",
    "modality_transform = {\n",
    "    c: transform_dict[c](model.modality_config[c]) for c in clip_type.keys()\n",
    "}\n",
    "\n",
    "device = \"cuda:0\"\n",
    "device = torch.device(device)\n",
    "model = model.to(device)\n",
    "model = model.half()\n",
    "\n",
    "video = [\n",
    "    \"semeval/experiments/kosenko/language_bind/LanguageBind/assets/video/0.mp4\",\n",
    "]\n",
    "\n",
    "\n",
    "language = [\n",
    "    \"A lion climbing a tree to catch a monkey.\",\n",
    "    \"Training a parrot to climb up a ladder.\",\n",
    "    \"Cute parrot is sitting on the floor.\",\n",
    "    \"Parrot climbs the small stairs.\",\n",
    "    \"Two pandas are eating bamboo.\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"video\": to_device(modality_transform[\"video\"](video), device),\n",
    "}\n",
    "\n",
    "\n",
    "inputs[\"language\"] = to_device(\n",
    "    tokenizer(\n",
    "        language,\n",
    "\n",
    "        max_length=77,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ),\n",
    "\n",
    "    device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "\n",
    "\n",
    "probs = torch.softmax(embeddings[\"video\"] @ embeddings[\"language\"].T, dim=-1)\n",
    "sorted_indices = probs.topk(k=len(language)).indices.squeeze().tolist()\n",
    "predicted_texts = \"\\n--\\n\".join([language[pos] for pos in sorted_indices])\n",
    "\n",
    "\n",
    "print(\"Video x Text: \\n\", probs)\n",
    "print(f\"Most similar text is: \\n{predicted_texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add language embeddings to bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from semeval.experiments.kosenko.language_bind.custom_bert import BertModel\n",
    "# from transformers import BertTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel._from_config(config)\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "# output = model(**encoded_input)\n",
    "# # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from semeval.experiments.kosenko.language_bind.custom_bert import (\n",
    "#     BertForCauseAnswering,\n",
    "# )\n",
    "# from transformers import BertTokenizer, AutoConfig\n",
    "\n",
    "# # model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertForCauseAnswering._from_config(\n",
    "#     AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# )\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BOS, U_1, U_2, U_3, ..., U_d, SEP, U_t, EOS]\n",
    "\n",
    "- BOS - вектор начала строки (в нашем случае он не имеет никакого отношения к оригинальному берт, инициализируется рандомно)\n",
    "- U_1 - вектор первой реплики\n",
    "- d - общее количество реплик (вроде как максимум тут 35)\n",
    "- SEP - разделительный вектор\n",
    "- U_t - вектор интересующей нас реплики для которой мы хотим предсказать причину\n",
    "- EOS - вектор конца строки\n",
    "\n",
    "Далее каждый вектор пропускается через линейный слой 768х2 и происходит бинарная классификация каждого токена. 0 это значит данная реплика не является причиной, 1 - является."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бейзлайн, который будет использовать архитектуру bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BOS, U_1, U_2, U_3, ..., U_d, SEP, U_t, SEP, U_c, EOS]\n",
    "\n",
    "- BOS - вектор начала строки (в нашем случае он не имеет никакого отношения к оригинальному берт, инициализируется рандомно)\n",
    "- U_1 - вектор первой реплики\n",
    "- d - общее количество реплик (вроде как максимум тут 35)\n",
    "- SEP - разделительный вектор\n",
    "- U_t - вектор интересующей нас реплики для которой мы хотим предсказать причины\n",
    "- U_c - реплика которая является причиной. Для отрицательных примеров вставляем ту, которая не является. \n",
    "- EOS - вектор конца строки\n",
    "\n",
    "Далее каждый вектор пропускается через линейный слой 768х1 и далее как в задаче squad мы предказываем позицию причины. В squad мы берем линейный слой 768х2, потом разрезаем получившийся вектор на 2 и потом предсказываем при помощи кросс энтропии позицию(класс) в строке.\n",
    "\n",
    "подобно тому как это сделано в bert for question answering\n",
    "```python\n",
    "outputs = self.bert(\n",
    "\tinput_ids,\n",
    "\tattention_mask=attention_mask,\n",
    "\ttoken_type_ids=token_type_ids,\n",
    "\tposition_ids=position_ids,\n",
    "\thead_mask=head_mask,\n",
    "\tinputs_embeds=inputs_embeds,\n",
    "\toutput_attentions=output_attentions,\n",
    "\toutput_hidden_states=output_hidden_states,\n",
    "\treturn_dict=return_dict,\n",
    ")\n",
    "\n",
    "sequence_output = outputs[0]\n",
    "# self.qa_outputs = nn.Linear(768, 2)\n",
    "logits = self.qa_outputs(sequence_output)\n",
    "# logits.shape = [batch, seq_len, 2]\n",
    "start_logits, end_logits = logits.split(1, dim=-1)\n",
    "# start_logits.shape = [batch, seq_len, 1]\n",
    "start_logits = start_logits.squeeze(-1).contiguous()\n",
    "end_logits = end_logits.squeeze(-1).contiguous()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/semeval_subtask2_conversations\")\n",
    "\n",
    "\n",
    "# dataset = dataset[\"train\"]\n",
    "# dataset\n",
    "def get_bert_cause_dataset(dataset):\n",
    "    new_dataset = []\n",
    "\n",
    "    for item in dataset:\n",
    "        conversation = item[\"conversation\"]\n",
    "        # print(item)\n",
    "        positive_pairs = []\n",
    "\n",
    "        for cause in item[\"emotion-cause_pairs\"]:\n",
    "            emotion_pos = int(cause[0].split(\"_\")[0]) - 1\n",
    "            emotion = cause[0].split(\"_\")[1]\n",
    "            cause_pos = int(cause[1]) - 1\n",
    "            # print(emotion_pos, emotion, cause_pos, cause)\n",
    "            new_dataset.append(\n",
    "                {\n",
    "                    \"conversation\": conversation,\n",
    "                    \"emotion_pos\": emotion_pos,\n",
    "                    \"cause_pos\": cause_pos,\n",
    "                    \"emotion_label\": emotion,\n",
    "                    \"label\": cause_pos,\n",
    "                }\n",
    "            )\n",
    "            positive_pairs.append((emotion_pos, cause_pos))\n",
    "\n",
    "        positive_pairs = set(positive_pairs)\n",
    "        negative_pairs = []\n",
    "        for pos_i in range(len(conversation)):\n",
    "            for pos_j in range(len(conversation)):\n",
    "                pair = (pos_i, pos_j)\n",
    "                if not pair in positive_pairs:\n",
    "                    negative_pairs.append(pair)\n",
    "\n",
    "        if len(negative_pairs) > len(positive_pairs):\n",
    "            negative_pairs = random.sample(negative_pairs, len(positive_pairs))\n",
    "\n",
    "        for pair in negative_pairs:\n",
    "            emotion = conversation[pair[0]][\"emotion\"]\n",
    "            new_dataset.append(\n",
    "                {\n",
    "                    \"conversation\": conversation,\n",
    "                    \"emotion_pos\": pair[0],\n",
    "                    \"cause_pos\": pair[1],\n",
    "                    \"emotion_label\": emotion,\n",
    "                    \"label\": -1,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "dataset_train = get_bert_cause_dataset(dataset=dataset[\"train\"])\n",
    "dataset_test = get_bert_cause_dataset(dataset=dataset[\"test\"])\n",
    "\n",
    "example = dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 768]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = []\n",
    "language = []\n",
    "base_video_path = \"/code/SemEval-2024_Task3/training_data/train\"\n",
    "for utterance in example[\"conversation\"]:\n",
    "    video_path = f\"{base_video_path}/{utterance['video_name']}\"\n",
    "    language_utt = utterance[\"text\"]\n",
    "    video.append(video_path)\n",
    "    language.append(language_utt)\n",
    "    # print(video_path)\n",
    "    # for utterance in item['conversation']:\n",
    "\n",
    "inputs = {\n",
    "    \"video\": to_device(\n",
    "        modality_transform[\"video\"](video),\n",
    "        device,\n",
    "    ),\n",
    "    \"language\": to_device(\n",
    "        tokenizer(\n",
    "            language,\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ),\n",
    "        device,\n",
    "    ),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "embeddings[\"language\"].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[\"language\"].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation': [{'emotion': 'sadness',\n",
       "   'speaker': 'Monica',\n",
       "   'text': 'Mr . Heckles .',\n",
       "   'utterance_ID': 1,\n",
       "   'video_name': 'dia187utt1.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Rachel',\n",
       "   'text': 'How did this happen ?',\n",
       "   'utterance_ID': 2,\n",
       "   'video_name': 'dia187utt2.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Mr. Treeger',\n",
       "   'text': 'He musta been sweeping . They found a broom in his hand .',\n",
       "   'utterance_ID': 3,\n",
       "   'video_name': 'dia187utt3.mp4'},\n",
       "  {'emotion': 'sadness',\n",
       "   'speaker': 'Monica',\n",
       "   'text': 'That is terrible .',\n",
       "   'utterance_ID': 4,\n",
       "   'video_name': 'dia187utt4.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Mr. Treeger',\n",
       "   'text': 'I know . I was sweeping yesterday . It coulda been me .',\n",
       "   'utterance_ID': 5,\n",
       "   'video_name': 'dia187utt5.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Ross',\n",
       "   'text': 'Sure , you coulda . You never know .',\n",
       "   'utterance_ID': 6,\n",
       "   'video_name': 'dia187utt6.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Mr. Treeger',\n",
       "   'text': 'You never know .',\n",
       "   'utterance_ID': 7,\n",
       "   'video_name': 'dia187utt7.mp4'}],\n",
       " 'emotion_pos': 0,\n",
       " 'cause_pos': 0,\n",
       " 'emotion_label': 'sadness',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_vector = []\n",
    "# PAD=0 CLS=1 SEP=2 BOS=4\n",
    "PAD = 0\n",
    "CLS = 1\n",
    "SEP = 2\n",
    "BOS = 3\n",
    "\n",
    "special_embeddings = torch.nn.Embedding(\n",
    "    4,\n",
    "    768,\n",
    "    padding_idx=0,\n",
    ")\n",
    "special_embeddings.cuda()\n",
    "\n",
    "new_vector.append(\n",
    "    special_embeddings(\n",
    "        torch.tensor(\n",
    "            [CLS],\n",
    "            device=\"cuda\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(embeddings[\"video\"].shape[0]):\n",
    "    video_vec = embeddings[\"video\"][i]\n",
    "    lang_vec = embeddings[\"language\"][i]\n",
    "    new_vector.append(video_vec)\n",
    "    new_vector.append(lang_vec)\n",
    "\n",
    "new_vector.append(\n",
    "    special_embeddings(\n",
    "        torch.tensor(\n",
    "            [SEP],\n",
    "            device=\"cuda\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "new_vector.append(\n",
    "    embeddings[\"video\"][example[\"emotion_pos\"]],\n",
    ")\n",
    "new_vector.append(\n",
    "    embeddings[\"language\"][example[\"emotion_pos\"]],\n",
    ")\n",
    "\n",
    "new_vector.append(\n",
    "    special_embeddings(\n",
    "        torch.tensor(\n",
    "            [SEP],\n",
    "            device=\"cuda\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "new_vector.append(\n",
    "    embeddings[\"video\"][example[\"cause_pos\"]],\n",
    ")\n",
    "new_vector.append(\n",
    "    embeddings[\"language\"][example[\"cause_pos\"]],\n",
    ")\n",
    "\n",
    "new_vector.append(\n",
    "    special_embeddings(\n",
    "        torch.tensor(\n",
    "            [BOS],\n",
    "            device=\"cuda\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "new_vector = torch.vstack(\n",
    "    new_vector,\n",
    ")\n",
    "new_vector = new_vector.unsqueeze(0)\n",
    "new_vector.shape\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vector.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semeval.experiments.kosenko.language_bind.custom_bert import (\n",
    "    BertForCauseAnswering,\n",
    ")\n",
    "from transformers import BertTokenizer, AutoConfig\n",
    "\n",
    "# model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForCauseAnswering._from_config(\n",
    "    AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    ")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([new_vector, new_vector], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.cuda()\n",
    "output = bert_model(\n",
    "    inputs_embeds=torch.cat(\n",
    "        [new_vector, new_vector],\n",
    "        dim=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "output.start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class CausePredictor(torch.nn.Module):\n",
    "    def __ini__(self, clip_type):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertForCauseAnswering._from_config(\n",
    "            AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        )\n",
    "        self.languagebind = LanguageBind(\n",
    "            clip_type=clip_type,\n",
    "            cache_dir=\"/code/cache_dir\",\n",
    "        )\n",
    "\n",
    "        self.PAD = 0\n",
    "        self.CLS = 1\n",
    "        self.SEP = 2\n",
    "        self.BOS = 3\n",
    "\n",
    "        self.special_embeddings = torch.nn.Embedding(\n",
    "            4,\n",
    "            768,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "    def get_embeddings_elem(self, dataset_item):\n",
    "        inputs = {\n",
    "            \"video\": to_device(\n",
    "                modality_transform[\"video\"](video),\n",
    "                device,\n",
    "            ),\n",
    "            \"language\": to_device(\n",
    "                tokenizer(\n",
    "                    language,\n",
    "                    max_length=77,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ),\n",
    "                device,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                embeddings = model(inputs)\n",
    "\n",
    "    def preprocess_elem(self, dataset_item):\n",
    "        new_vector = []\n",
    "        device = self.special_embeddings.device\n",
    "\n",
    "        new_vector.append(\n",
    "            self.special_embeddings(\n",
    "                torch.tensor(\n",
    "                    [self.CLS],\n",
    "                    device=device,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(embeddings[\"video\"].shape[0]):\n",
    "            video_vec = embeddings[\"video\"][i]\n",
    "            lang_vec = embeddings[\"language\"][i]\n",
    "            new_vector.append(video_vec)\n",
    "            new_vector.append(lang_vec)\n",
    "\n",
    "        new_vector.append(\n",
    "            special_embeddings(\n",
    "                torch.tensor(\n",
    "                    [self.SEP],\n",
    "                    device=device,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        new_vector.append(\n",
    "            embeddings[\"video\"][dataset_item[\"emotion_pos\"]],\n",
    "        )\n",
    "        new_vector.append(\n",
    "            embeddings[\"language\"][dataset_item[\"emotion_pos\"]],\n",
    "        )\n",
    "\n",
    "        new_vector.append(\n",
    "            special_embeddings(\n",
    "                torch.tensor(\n",
    "                    [self.SEP],\n",
    "                    device=device,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        new_vector.append(\n",
    "            embeddings[\"video\"][dataset_item[\"cause_pos\"]],\n",
    "        )\n",
    "        new_vector.append(\n",
    "            embeddings[\"language\"][dataset_item[\"cause_pos\"]],\n",
    "        )\n",
    "\n",
    "        new_vector.append(\n",
    "            self.special_embeddings(\n",
    "                torch.tensor(\n",
    "                    [self.BOS],\n",
    "                    device=device,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        new_vector = torch.vstack(\n",
    "            new_vector,\n",
    "        )\n",
    "        # new_vector = new_vector.unsqueeze(0)\n",
    "        # new_vector.shape\n",
    "        return new_vector\n",
    "\n",
    "    def tokenize(self, batch):\n",
    "        pass\n",
    "\n",
    "    def forward(self, batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation': [{'emotion': 'sadness',\n",
       "   'speaker': 'Monica',\n",
       "   'text': 'Mr . Heckles .',\n",
       "   'utterance_ID': 1,\n",
       "   'video_name': 'dia187utt1.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Rachel',\n",
       "   'text': 'How did this happen ?',\n",
       "   'utterance_ID': 2,\n",
       "   'video_name': 'dia187utt2.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Mr. Treeger',\n",
       "   'text': 'He musta been sweeping . They found a broom in his hand .',\n",
       "   'utterance_ID': 3,\n",
       "   'video_name': 'dia187utt3.mp4'},\n",
       "  {'emotion': 'sadness',\n",
       "   'speaker': 'Monica',\n",
       "   'text': 'That is terrible .',\n",
       "   'utterance_ID': 4,\n",
       "   'video_name': 'dia187utt4.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Mr. Treeger',\n",
       "   'text': 'I know . I was sweeping yesterday . It coulda been me .',\n",
       "   'utterance_ID': 5,\n",
       "   'video_name': 'dia187utt5.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Ross',\n",
       "   'text': 'Sure , you coulda . You never know .',\n",
       "   'utterance_ID': 6,\n",
       "   'video_name': 'dia187utt6.mp4'},\n",
       "  {'emotion': 'neutral',\n",
       "   'speaker': 'Mr. Treeger',\n",
       "   'text': 'You never know .',\n",
       "   'utterance_ID': 7,\n",
       "   'video_name': 'dia187utt7.mp4'}],\n",
       " 'emotion_pos': 0,\n",
       " 'cause_pos': 0,\n",
       " 'emotion_label': 'sadness',\n",
       " 'label': 0,\n",
       " 'video_base_path': '/code/SemEval-2024_Task3/training_data/train'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BERTCauseConversationsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conversations,\n",
    "        base_video_path=\"/code/SemEval-2024_Task3/training_data/train\",\n",
    "    ):\n",
    "        self.conversations = conversations\n",
    "\n",
    "        self.base_video_path = base_video_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        turn = self.conversations[idx]\n",
    "\n",
    "        turn[\"video_base_path\"] = self.base_video_path\n",
    "\n",
    "        return turn\n",
    "\n",
    "\n",
    "train_dataset = BERTCauseConversationsDataset(conversations=dataset_train)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
