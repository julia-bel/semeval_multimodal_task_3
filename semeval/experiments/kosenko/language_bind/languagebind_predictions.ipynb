{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/code/semeval/experiments/kosenko/language_bind/LanguageBind/languagebind/audio/processing_audio.py:17: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "from semeval.experiments.kosenko.language_bind.LanguageBind.languagebind import (\n",
    "    LanguageBind,\n",
    "    to_device,\n",
    "    transform_dict,\n",
    "    LanguageBindImageTokenizer,\n",
    ")\n",
    "from semeval.experiments.kosenko.language_bind.languagebind_classification_video_text import (\n",
    "    CauseVideoTextClassif,\n",
    ")\n",
    "\n",
    "\n",
    "all_emotions = [\n",
    "    \"surprise\",\n",
    "    \"fear\",\n",
    "    \"sadness\",\n",
    "    \"neutral\",\n",
    "    \"joy\",\n",
    "    \"anger\",\n",
    "    \"disgust\",\n",
    "]\n",
    "\n",
    "emotions2labels = {em: i for i, em in enumerate(all_emotions)}\n",
    "\n",
    "labels2emotions = {i: em for i, em in enumerate(all_emotions)}\n",
    "\n",
    "clip_type = {\n",
    "    \"video\": \"LanguageBind_Video_FT\",\n",
    "}\n",
    "\n",
    "model = CauseVideoTextClassif(\n",
    "    labels=len(all_emotions),\n",
    "    clip_type=clip_type,\n",
    ")\n",
    "\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"semeval/experiments/kosenko/language_bind/train_results/exp_9_checkpoint-6703/pytorch_model.bin\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pretrained_ckpt = f\"LanguageBind/LanguageBind_Image\"\n",
    "tokenizer = LanguageBindImageTokenizer.from_pretrained(\n",
    "    pretrained_ckpt, cache_dir=\"/code/cache_dir/tokenizer_cache_dir\"\n",
    ")\n",
    "modality_transform = {\n",
    "    c: transform_dict[c](model.model.modality_config[c]) for c in clip_type.keys()\n",
    "}\n",
    "\n",
    "device = \"cuda:0\"\n",
    "device = torch.device(device)\n",
    "model = model.to(device)\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = [\n",
    "    \"semeval/experiments/kosenko/language_bind/LanguageBind/assets/video/0.mp4\",\n",
    "]\n",
    "language = [\n",
    "    \"Two pandas are eating bamboo.\",\n",
    "]\n",
    "\n",
    "custom_inputs = {\n",
    "    \"initial_video\": to_device(\n",
    "        modality_transform[\"video\"](video),\n",
    "        device,\n",
    "    ),\n",
    "    \"cause_video\": to_device(\n",
    "        modality_transform[\"video\"](video),\n",
    "        device,\n",
    "    ),\n",
    "    \"initial_language\": to_device(\n",
    "        tokenizer(\n",
    "            language,\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ),\n",
    "        device,\n",
    "    ),\n",
    "    \"cause_language\": to_device(\n",
    "        tokenizer(\n",
    "            language,\n",
    "            max_length=77,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ),\n",
    "        device,\n",
    "    ),\n",
    "}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        result = model(custom_inputs)\n",
    "result[0].argmax(-1).item(), result[1].argmax(-1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dim/semeval_subtask2_conversations\")\n",
    "dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_ID': 1231,\n",
       " 'conversation': [{'emotion': 'neutral',\n",
       "   'speaker': 'Phoebe',\n",
       "   'text': 'No .',\n",
       "   'utterance_ID': 1,\n",
       "   'video_name': 'dia1231utt1.mp4'},\n",
       "  {'emotion': 'anger',\n",
       "   'speaker': 'Phoebe',\n",
       "   'text': 'No !',\n",
       "   'utterance_ID': 2,\n",
       "   'video_name': 'dia1231utt2.mp4'},\n",
       "  {'emotion': 'joy',\n",
       "   'speaker': 'Phoebe',\n",
       "   'text': 'Oh , would you look at that Monica ?',\n",
       "   'utterance_ID': 3,\n",
       "   'video_name': 'dia1231utt3.mp4'},\n",
       "  {'emotion': 'joy',\n",
       "   'speaker': 'Phoebe',\n",
       "   'text': 'I just knocked off all of your top scores , how sad .',\n",
       "   'utterance_ID': 4,\n",
       "   'video_name': 'dia1231utt4.mp4'},\n",
       "  {'emotion': 'anger',\n",
       "   'speaker': 'Monica',\n",
       "   'text': 'Okay , I am next . Do not ! Do not start another game ! I said I am next ! Phoebe !',\n",
       "   'utterance_ID': 5,\n",
       "   'video_name': 'dia1231utt5.mp4'},\n",
       "  {'emotion': 'joy',\n",
       "   'speaker': 'Phoebe',\n",
       "   'text': 'Oh , I am sorry . I did not hear you over all the winning .',\n",
       "   'utterance_ID': 6,\n",
       "   'video_name': 'dia1231utt6.mp4'},\n",
       "  {'emotion': 'anger',\n",
       "   'speaker': 'Monica',\n",
       "   'text': 'Chandler ! Phoebe hogging the game !',\n",
       "   'utterance_ID': 7,\n",
       "   'video_name': 'dia1231utt7.mp4'}],\n",
       " 'emotion-cause_pairs': [['2_anger', '2'],\n",
       "  ['3_joy', '4'],\n",
       "  ['4_joy', '4'],\n",
       "  ['5_anger', '5'],\n",
       "  ['6_joy', '4'],\n",
       "  ['6_joy', '6'],\n",
       "  ['7_anger', '7']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sadness 6 1\n",
      "1 sadness 7 1\n",
      "2 anger 1 1\n",
      "2 anger 3 1\n",
      "2 anger 4 1\n",
      "2 anger 5 1\n",
      "2 anger 6 1\n",
      "2 anger 7 1\n",
      "3 surprise 1 1\n",
      "3 surprise 2 1\n",
      "3 surprise 3 1\n",
      "3 surprise 4 1\n",
      "3 surprise 5 1\n",
      "3 surprise 6 1\n",
      "3 surprise 7 1\n",
      "5 anger 1 1\n",
      "5 anger 3 1\n",
      "5 anger 4 1\n",
      "5 anger 5 1\n",
      "5 anger 6 1\n",
      "5 anger 7 1\n",
      "6 anger 1 1\n",
      "6 anger 2 1\n",
      "6 anger 3 1\n",
      "6 anger 4 1\n",
      "6 anger 5 1\n",
      "6 anger 6 1\n",
      "6 anger 7 1\n",
      "7 anger 3 1\n",
      "7 anger 5 1\n",
      "7 anger 6 1\n",
      "7 anger 7 1\n"
     ]
    }
   ],
   "source": [
    "for conv in dataset:\n",
    "    conversation = conv[\"conversation\"]\n",
    "    for i in range(len(conversation)):\n",
    "        item_i = conversation[i]\n",
    "        for j in range(len(conversation)):\n",
    "            item_j = conversation[j]\n",
    "            base_path = \"/code/SemEval-2024_Task3/training_data/train\"\n",
    "            initial_video = [\n",
    "                f'{base_path}/{item_i[\"video_name\"]}',\n",
    "            ]\n",
    "            cause_video = [\n",
    "                f'{base_path}/{item_j[\"video_name\"]}',\n",
    "            ]\n",
    "            initial_language = [\n",
    "                item_i[\"text\"],\n",
    "            ]\n",
    "            cause_language = [\n",
    "                item_j[\"text\"],\n",
    "            ]\n",
    "\n",
    "            custom_inputs = {\n",
    "                \"initial_video\": to_device(\n",
    "                    modality_transform[\"video\"](video),\n",
    "                    device,\n",
    "                ),\n",
    "                \"cause_video\": to_device(\n",
    "                    modality_transform[\"video\"](video),\n",
    "                    device,\n",
    "                ),\n",
    "                \"initial_language\": to_device(\n",
    "                    tokenizer(\n",
    "                        initial_language,\n",
    "                        max_length=77,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                    ),\n",
    "                    device,\n",
    "                ),\n",
    "                \"cause_language\": to_device(\n",
    "                    tokenizer(\n",
    "                        cause_language,\n",
    "                        max_length=77,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                    ),\n",
    "                    device,\n",
    "                ),\n",
    "            }\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=\"cuda\"):\n",
    "                    result = model(custom_inputs)\n",
    "            emotion = result[0].argmax(-1).item()\n",
    "            emotion = labels2emotions[emotion]\n",
    "            if emotion != \"neutral\":\n",
    "                cause_or_not = result[1].argmax(-1).item()\n",
    "                if cause_or_not == 1:\n",
    "                    print(i + 1, emotion, j + 1, cause_or_not)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['3_joy', '4'],\n",
    "['4_joy', '4'],\n",
    "['5_anger', '5'],\n",
    "['6_joy', '4'],\n",
    "['6_joy', '6'],\n",
    "['7_anger', '7']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
