{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "'''\n",
    "Using the emotion representation model\n",
    "rec_result only contains {'feats'}\n",
    "\tgranularity=\"utterance\": {'feats': [*768]}\n",
    "\tgranularity=\"frame\": {feats: [T*768]}\n",
    "'''\n",
    "model = AutoModel(model=\"iic/emotion2vec_base\", model_revision=\"v2.0.4\")\n",
    "wav_file = f\"{model.model_path}/example/test.wav\"\n",
    "rec_result = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\")\n",
    "print(res)\n",
    "\n",
    "'''\n",
    "Using the finetuned emotion recognization model\n",
    "rec_result contains {'feats', 'labels', 'scores'}\n",
    "\textract_embedding=False: 8-class emotions with scores\n",
    "\textract_embedding=True: 8-class emotions with scores, along with features\n",
    "\n",
    "8-class emotions:\n",
    "    0: angry\n",
    "    1: disgusted\n",
    "    2: fearful\n",
    "    3: happy\n",
    "    4: neutral\n",
    "    5: other\n",
    "    6: sad\n",
    "    7: surprised\n",
    "    8: unknown\n",
    "'''\n",
    "model = AutoModel(model=\"iic/emotion2vec_base_finetuned\", model_revision=\"v2.0.4\")\n",
    "wav_file = f\"{model.model_path}/example/test.wav\"\n",
    "rec_result = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
