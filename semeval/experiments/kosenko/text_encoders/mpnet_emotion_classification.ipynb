{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 2.51MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 1.24MB/s]\n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 109MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 628kB/s]\n",
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 6.82MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:01<00:00, 316MB/s]  \n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 2.04MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.06MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.38MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 1.23MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02250257 -0.07829171 -0.02303073 ... -0.00827928  0.02652688\n",
      "  -0.00201897]\n",
      " [ 0.04170235  0.00109741 -0.0155342  ... -0.02181629 -0.06359357\n",
      "  -0.00875285]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at sentence-transformers/all-mpnet-base-v2 were not used when initializing MPNetForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MPNetForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = MPNetForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0146,  0.0233,  0.0383, -0.0608,  0.0067]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(['test'], return_tensors='pt')\n",
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpnet.embeddings.word_embeddings.weight\n",
      "mpnet.embeddings.position_embeddings.weight\n",
      "mpnet.embeddings.LayerNorm.weight\n",
      "mpnet.embeddings.LayerNorm.bias\n",
      "mpnet.encoder.layer.0.attention.attn.q.weight\n",
      "mpnet.encoder.layer.0.attention.attn.q.bias\n",
      "mpnet.encoder.layer.0.attention.attn.k.weight\n",
      "mpnet.encoder.layer.0.attention.attn.k.bias\n",
      "mpnet.encoder.layer.0.attention.attn.v.weight\n",
      "mpnet.encoder.layer.0.attention.attn.v.bias\n",
      "mpnet.encoder.layer.0.attention.attn.o.weight\n",
      "mpnet.encoder.layer.0.attention.attn.o.bias\n",
      "mpnet.encoder.layer.0.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.0.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.0.intermediate.dense.weight\n",
      "mpnet.encoder.layer.0.intermediate.dense.bias\n",
      "mpnet.encoder.layer.0.output.dense.weight\n",
      "mpnet.encoder.layer.0.output.dense.bias\n",
      "mpnet.encoder.layer.0.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.0.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.1.attention.attn.q.weight\n",
      "mpnet.encoder.layer.1.attention.attn.q.bias\n",
      "mpnet.encoder.layer.1.attention.attn.k.weight\n",
      "mpnet.encoder.layer.1.attention.attn.k.bias\n",
      "mpnet.encoder.layer.1.attention.attn.v.weight\n",
      "mpnet.encoder.layer.1.attention.attn.v.bias\n",
      "mpnet.encoder.layer.1.attention.attn.o.weight\n",
      "mpnet.encoder.layer.1.attention.attn.o.bias\n",
      "mpnet.encoder.layer.1.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.1.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.1.intermediate.dense.weight\n",
      "mpnet.encoder.layer.1.intermediate.dense.bias\n",
      "mpnet.encoder.layer.1.output.dense.weight\n",
      "mpnet.encoder.layer.1.output.dense.bias\n",
      "mpnet.encoder.layer.1.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.1.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.2.attention.attn.q.weight\n",
      "mpnet.encoder.layer.2.attention.attn.q.bias\n",
      "mpnet.encoder.layer.2.attention.attn.k.weight\n",
      "mpnet.encoder.layer.2.attention.attn.k.bias\n",
      "mpnet.encoder.layer.2.attention.attn.v.weight\n",
      "mpnet.encoder.layer.2.attention.attn.v.bias\n",
      "mpnet.encoder.layer.2.attention.attn.o.weight\n",
      "mpnet.encoder.layer.2.attention.attn.o.bias\n",
      "mpnet.encoder.layer.2.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.2.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.2.intermediate.dense.weight\n",
      "mpnet.encoder.layer.2.intermediate.dense.bias\n",
      "mpnet.encoder.layer.2.output.dense.weight\n",
      "mpnet.encoder.layer.2.output.dense.bias\n",
      "mpnet.encoder.layer.2.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.2.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.3.attention.attn.q.weight\n",
      "mpnet.encoder.layer.3.attention.attn.q.bias\n",
      "mpnet.encoder.layer.3.attention.attn.k.weight\n",
      "mpnet.encoder.layer.3.attention.attn.k.bias\n",
      "mpnet.encoder.layer.3.attention.attn.v.weight\n",
      "mpnet.encoder.layer.3.attention.attn.v.bias\n",
      "mpnet.encoder.layer.3.attention.attn.o.weight\n",
      "mpnet.encoder.layer.3.attention.attn.o.bias\n",
      "mpnet.encoder.layer.3.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.3.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.3.intermediate.dense.weight\n",
      "mpnet.encoder.layer.3.intermediate.dense.bias\n",
      "mpnet.encoder.layer.3.output.dense.weight\n",
      "mpnet.encoder.layer.3.output.dense.bias\n",
      "mpnet.encoder.layer.3.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.3.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.4.attention.attn.q.weight\n",
      "mpnet.encoder.layer.4.attention.attn.q.bias\n",
      "mpnet.encoder.layer.4.attention.attn.k.weight\n",
      "mpnet.encoder.layer.4.attention.attn.k.bias\n",
      "mpnet.encoder.layer.4.attention.attn.v.weight\n",
      "mpnet.encoder.layer.4.attention.attn.v.bias\n",
      "mpnet.encoder.layer.4.attention.attn.o.weight\n",
      "mpnet.encoder.layer.4.attention.attn.o.bias\n",
      "mpnet.encoder.layer.4.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.4.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.4.intermediate.dense.weight\n",
      "mpnet.encoder.layer.4.intermediate.dense.bias\n",
      "mpnet.encoder.layer.4.output.dense.weight\n",
      "mpnet.encoder.layer.4.output.dense.bias\n",
      "mpnet.encoder.layer.4.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.4.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.5.attention.attn.q.weight\n",
      "mpnet.encoder.layer.5.attention.attn.q.bias\n",
      "mpnet.encoder.layer.5.attention.attn.k.weight\n",
      "mpnet.encoder.layer.5.attention.attn.k.bias\n",
      "mpnet.encoder.layer.5.attention.attn.v.weight\n",
      "mpnet.encoder.layer.5.attention.attn.v.bias\n",
      "mpnet.encoder.layer.5.attention.attn.o.weight\n",
      "mpnet.encoder.layer.5.attention.attn.o.bias\n",
      "mpnet.encoder.layer.5.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.5.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.5.intermediate.dense.weight\n",
      "mpnet.encoder.layer.5.intermediate.dense.bias\n",
      "mpnet.encoder.layer.5.output.dense.weight\n",
      "mpnet.encoder.layer.5.output.dense.bias\n",
      "mpnet.encoder.layer.5.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.5.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.6.attention.attn.q.weight\n",
      "mpnet.encoder.layer.6.attention.attn.q.bias\n",
      "mpnet.encoder.layer.6.attention.attn.k.weight\n",
      "mpnet.encoder.layer.6.attention.attn.k.bias\n",
      "mpnet.encoder.layer.6.attention.attn.v.weight\n",
      "mpnet.encoder.layer.6.attention.attn.v.bias\n",
      "mpnet.encoder.layer.6.attention.attn.o.weight\n",
      "mpnet.encoder.layer.6.attention.attn.o.bias\n",
      "mpnet.encoder.layer.6.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.6.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.6.intermediate.dense.weight\n",
      "mpnet.encoder.layer.6.intermediate.dense.bias\n",
      "mpnet.encoder.layer.6.output.dense.weight\n",
      "mpnet.encoder.layer.6.output.dense.bias\n",
      "mpnet.encoder.layer.6.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.6.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.7.attention.attn.q.weight\n",
      "mpnet.encoder.layer.7.attention.attn.q.bias\n",
      "mpnet.encoder.layer.7.attention.attn.k.weight\n",
      "mpnet.encoder.layer.7.attention.attn.k.bias\n",
      "mpnet.encoder.layer.7.attention.attn.v.weight\n",
      "mpnet.encoder.layer.7.attention.attn.v.bias\n",
      "mpnet.encoder.layer.7.attention.attn.o.weight\n",
      "mpnet.encoder.layer.7.attention.attn.o.bias\n",
      "mpnet.encoder.layer.7.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.7.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.7.intermediate.dense.weight\n",
      "mpnet.encoder.layer.7.intermediate.dense.bias\n",
      "mpnet.encoder.layer.7.output.dense.weight\n",
      "mpnet.encoder.layer.7.output.dense.bias\n",
      "mpnet.encoder.layer.7.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.7.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.8.attention.attn.q.weight\n",
      "mpnet.encoder.layer.8.attention.attn.q.bias\n",
      "mpnet.encoder.layer.8.attention.attn.k.weight\n",
      "mpnet.encoder.layer.8.attention.attn.k.bias\n",
      "mpnet.encoder.layer.8.attention.attn.v.weight\n",
      "mpnet.encoder.layer.8.attention.attn.v.bias\n",
      "mpnet.encoder.layer.8.attention.attn.o.weight\n",
      "mpnet.encoder.layer.8.attention.attn.o.bias\n",
      "mpnet.encoder.layer.8.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.8.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.8.intermediate.dense.weight\n",
      "mpnet.encoder.layer.8.intermediate.dense.bias\n",
      "mpnet.encoder.layer.8.output.dense.weight\n",
      "mpnet.encoder.layer.8.output.dense.bias\n",
      "mpnet.encoder.layer.8.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.8.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.9.attention.attn.q.weight\n",
      "mpnet.encoder.layer.9.attention.attn.q.bias\n",
      "mpnet.encoder.layer.9.attention.attn.k.weight\n",
      "mpnet.encoder.layer.9.attention.attn.k.bias\n",
      "mpnet.encoder.layer.9.attention.attn.v.weight\n",
      "mpnet.encoder.layer.9.attention.attn.v.bias\n",
      "mpnet.encoder.layer.9.attention.attn.o.weight\n",
      "mpnet.encoder.layer.9.attention.attn.o.bias\n",
      "mpnet.encoder.layer.9.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.9.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.9.intermediate.dense.weight\n",
      "mpnet.encoder.layer.9.intermediate.dense.bias\n",
      "mpnet.encoder.layer.9.output.dense.weight\n",
      "mpnet.encoder.layer.9.output.dense.bias\n",
      "mpnet.encoder.layer.9.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.9.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.10.attention.attn.q.weight\n",
      "mpnet.encoder.layer.10.attention.attn.q.bias\n",
      "mpnet.encoder.layer.10.attention.attn.k.weight\n",
      "mpnet.encoder.layer.10.attention.attn.k.bias\n",
      "mpnet.encoder.layer.10.attention.attn.v.weight\n",
      "mpnet.encoder.layer.10.attention.attn.v.bias\n",
      "mpnet.encoder.layer.10.attention.attn.o.weight\n",
      "mpnet.encoder.layer.10.attention.attn.o.bias\n",
      "mpnet.encoder.layer.10.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.10.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.10.intermediate.dense.weight\n",
      "mpnet.encoder.layer.10.intermediate.dense.bias\n",
      "mpnet.encoder.layer.10.output.dense.weight\n",
      "mpnet.encoder.layer.10.output.dense.bias\n",
      "mpnet.encoder.layer.10.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.10.output.LayerNorm.bias\n",
      "mpnet.encoder.layer.11.attention.attn.q.weight\n",
      "mpnet.encoder.layer.11.attention.attn.q.bias\n",
      "mpnet.encoder.layer.11.attention.attn.k.weight\n",
      "mpnet.encoder.layer.11.attention.attn.k.bias\n",
      "mpnet.encoder.layer.11.attention.attn.v.weight\n",
      "mpnet.encoder.layer.11.attention.attn.v.bias\n",
      "mpnet.encoder.layer.11.attention.attn.o.weight\n",
      "mpnet.encoder.layer.11.attention.attn.o.bias\n",
      "mpnet.encoder.layer.11.attention.LayerNorm.weight\n",
      "mpnet.encoder.layer.11.attention.LayerNorm.bias\n",
      "mpnet.encoder.layer.11.intermediate.dense.weight\n",
      "mpnet.encoder.layer.11.intermediate.dense.bias\n",
      "mpnet.encoder.layer.11.output.dense.weight\n",
      "mpnet.encoder.layer.11.output.dense.bias\n",
      "mpnet.encoder.layer.11.output.LayerNorm.weight\n",
      "mpnet.encoder.layer.11.output.LayerNorm.bias\n",
      "mpnet.encoder.relative_attention_bias.weight\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "\tprint(param[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
